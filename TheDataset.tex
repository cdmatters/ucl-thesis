\chapter{The Dataset}
\label{the_dataset}

% \begin{enumerate}
%     \item The dataset used in this investigation is original, and presents a new opportunity, not seen in the literature thus far.
%     \item In particular it offers a close alignment between features of code, and descriptive elements that aim to tell you the function of the specific code element - distinct from the whole function.
%     \item In this section we will present the means of collection, the structure of the dataset, and finally an analysis of its composition. 
%     \item This analysis aims to be informative by both being quantitative, highlight the statistical composition of the dataset, and qualitative.
%     \item Finally we  also compare it and contrast it to other datasets.
% \end{enumerate}


The dataset used in the our investigation was entirely original, presenting a new opportunity to investigate the machine translation of code not seen in the literature thus far. \textbf{CITE}
In particular it offers a close and granular alignment between elements of code (argument names) and their natural language descriptions, as extracted from the most popular open source Python libraries.
In this section we present first an overview of the dataset, and elaborate on  the means of collection. 
We then present an in-depth analysis of its structure and composition, which aims to be both quantitative, and qualitive, comparing the dataset to others in the field. 

\section{Overview} % (fold)
\label{sec:overview}

The dataset comprises of a list of Python function arguments, their natural language descriptions and surrounding metadata. 
This metadata is extensive, containing numerous fields including the source code of the function, function name and filename, among others. 
The full structure of the data fields is enumerated in Table \ref{table:metadata}. 

This data is sourced exclusively from the top 300 most downloaded libraries on "PyPI", the python package manager. 
This ensured that the libraries were both good quality, well documented, and very much ``real world code.''
The data was then stored in YAML format, for human readability. 
A real example is presented in Appendix \ref{example_datapoint}, Listing \ref{lst:single_point}


\begin{table}[h!]
    \begin{center}
    \begin{tabular}{| c | c | c | c |}
    \hline
        Name &  Description     &    Field    & Type  \\
    \hline
        Argument Name & the name of the argument  & arg\_name & string \\
        Argument Description & the description of the argument & arg\_desc & string \\
        Argument Type & the annotated type for the argument & arg\_type & string or null \\
        All Arguments & all arguments used in the function & args & list of strings \\
        Other Argument Info & name, desc. and type for all other arguments & arg\_info & dict of dicts\\
        Function Name & the name of the function & name & string\\
        Signature & the function signature & sig & string\\
        Path & the full path to the file & filename & string \\
        Library & the library of the function & pkg & string\\
        Source & the source code of the function & src & string\\
        Docstring & the sphinx annotated docstring & docstring & string\\

    \hline


    \end{tabular}
    \caption {Fields collected for each single datapoint. Field refers to the actual key used in YAML file, and Type indicates the type of the data as stored in the file.}
    \label{table:metadata}
    \end{center}
\end{table}


% section overview (end)

\section{Method of Collection} % (fold)
\label{sec:method_of_collection}

\begin{enumerate}
    \item Python is a dynamically typed language, with emphasis on documentation, and naturalness.
    \item Sphinx is the industry standard for automating documentation generation.
    \item It contains an open source plugin `napoleon' that generate html web API documentation for docstrings formatted ether according to the Google Standard or the Numpy Standard (show examples of both).
    \item This appears to be used extensively in industry by xyz
    \item We wrote a plugin forked off napoleon that instead generated parsed the data and wrote it to an output yaml file (CITE)
    \item Since we require libaries with good documentation, that are easy to install (for the plug in to read them - documentation is sources by syntax tree??) we decided to run on the most popular libraries in python. we used the pip wall of superpowers to search for 300 most popular libraries on pip.
    \item After collecting these, we preprocessed by removing arguments without description (ie self), and those without code bodies (Ie (not implemented error) in abstract base class)
    \item more on filtering
\end{enumerate}



% section structure_of_dataset (end)

\section{Composition of the Dataset} % (fold)
\label{sec:composition_of_the_dataset}

The total list of libraries scraped is available as an appendix, with a full break down of composition. Below are the libraries largely taken from. 

\begin{table}[h!]
    \begin{center}
    \begin{tabular}{c | c | c | c}
        Library      & $ Arguments $     & $ Functions $ & Scientific \\
    \hline
        tensorflow   & $ 17002 $     & $ 4340 $ & True \\
        google   & $ 2829 $      & $ 1098 $ & False \\
        scipy    & $ 2030 $      & $ 549 $ & True \\
        networkx     & $ 1869 $      & $ 669 $ & True \\
        matplotlib   & $ 1457 $      & $ 366 $ & True \\
        sklearn      & $ 1403 $      & $ 368 $ & True \\
        pandas   & $ 1317 $      & $ 419 $ & True \\
        magenta      & $ 1045 $      & $ 357 $ & False \\
        (\textit{100 $<$ arguments $<$ 1000})   & $ 10172 $     & $ 3137 $ & True: 6, False: 26 \\
        (\textit{0 $<$ arguments $<$ 100})      & $ 2300 $      & $ 1036 $ & True: 9, False: 84 \\
    \hline
    \hline
        TOTAL    & $ 41424 $     & $ 12339 $ & True:21, False: 112 \\
    \end{tabular}
    \caption {Break down of the largest libraries}
    \label{table:breakdown_by_library}
    \end{center}
\end{table}
\section{Analysis of Data} % (fold)
\label{sec:analysis_of_data}


\subsection{Statistical Analysis of Natural Language} % (fold)
\label{sub:statistical_analysis_of_natural_language}

\begin{enumerate}
    \item variable name frequencies
    \item function name frequencies
    \item description lengths
    \item duplicates 
    \item tf-idf
    \item descpriptions per arg
\end{enumerate}

% subsection statistical_analysis_of_natura_language (end)

\subsection{Statistical Analysis of Code} % (fold)
\label{sub:statistical_analysis_of_code}

\begin{enumerate}
    \item path frequencies
    \item paths per point
\end{enumerate}

% subsection statistical_analysis_of_code (end)

\subsection{Qualitative Analysis of Dataset} % (fold)
\label{sub:qualitative_analysis_}

tfidf
% subsection qualitative_analysis_ (end)
% section analysis_of_data (end)

\subsection{Comparison To Other Datasets} % (fold)
\label{sub:comparison_to_other_datasets}

    This should be contrasted with comments, and overall docstrings, and full annotations.
    \begin{itemize}
        \item Comments data set - often comment is extra information when reading code. Why - not what
        \item Docstrings - have much more context and can be unspecific. It is unclear what partof the code does what. The mapping is very unclear
        \item Annotations - much more granular and good mapping of meaning to description, but not organic necessarily. DIfficult to source large dataset in.
    \end{itemize}


\section{Our Final Datasets} % (fold)
\label{sec:our_final_datasets}

A table of the final datasets used in the experiment (duplicates filtered or not, split vs unsplit)


% subsection comparison_to_other_datasets (end)