\chapter{Introduction}
\label{chapterlabel1}


% Paragraph on what this chapter does or will do:
% \begin{enumerate}
%     \item Introduce the problem
%     \item Outline motivation for the problem 
%     \item Outline questions and objectives of the thesis
%     \item Present the structure of the Thesis 
% \end{enumerate}


In this chapter we motivate our investigation into the documentation generation of fine-grained elements of code, present the central problem in question, and formalise the objectives of this investigation and report. 
Finally we present an overview of the contributions of our research and the following structure of the thesis.



\section{Motivation} % (fold)
\label{sec:motivation}

% Given the prominence that computers and digital infrastructure take in modern life, it is not hard to motivate work to help humans communicate with machines or vice versa. 
Source code is the purest medium through which humans communicate their instructions to computers.
Yet these instructions need to be comprehensible to other humans as well.
Understanding source code is already a valuable skill in industry today and one that is likely to grow in importance, given the prominence of computers in modern life.
% as the number of people with such skills fails to match demand. 
Work that facilitates human understanding of source code is therefore highly likely to find value, in industry and elsewhere. % where such communication through such a medium is vital.

Source code documentation is the textual component of code, designed to provide clear communication to human readers.
It is often an abstract summary of the purpose or function of the code, though sometimes an overview of the underlying subroutine.
Such summaries prove valuable in their own right, but are only one aid in understanding the source code they relate to.
Often an understanding of the individual, fine-grained elements of the source code requires investigation by the reader, or an explicit mention by the documentation writer.

With the advent of dynamically-typed languages, which do not make explicit information such as `integer', or `string', explanations of individual elements of source code, such as arguments, have renewed importance.
In fact, a number of open-source projects \cite{noauthor_numpydoc_nodate} and industrial company guidelines \cite{noauthor_style_2018} now require an explicit description of the arguments of each function in contributions to their projects.
The constant enumeration of these is manual, tedious, and prone to error, inaccuracy (through code churn) or even neglect.
% Furthermore, these descriptions are increasingly used to generate static websites that officially document code, so an outdated description 

As a result an algorithm that could automatically provide human descriptions of arguments would find great use in software tooling, either to help code readers understand undocumented code, or to help code writers automate their task.
Such an algorithm would also be a stepping stone to providing full descritions of all elements in the source code, further helping humans with code comprehension.
Given the advances that industry hope to make in the fields of automatic program synthesis, such tools will be vital to help humans understand
the computer output.
Lastly such a tool would be a stepping stone to the solving problem of type inference in dynamically-typed languages, which would help software developers prevent critical run-time failures in large scale software.



\section{Code \& The Naturalness Hypothesis} % (fold)
\label{sec:code_and_natural_language}

% The area of machine translation is a branch of natural language processing that has seen a dramatic improvement in recent decades. \textbf{CITE}
% The proliferation of `deep learning' techniqes and the availability of the large datasets have led to a series of models that have recently surpassed human ability on \textbf{CITE}. % ANOTHER SENTENCE
% However, most of advances have come in the field of `natural language' - that is language as spoken, read and written by humans. 
% Other modalities of communication - such as source code, as interpretted by computers - have, until recently, not been the subject of as much investigation by natual language research community.


% \begin{itemize}
%     \item Historically how was code treated in research commnuity
%     \item Nowadays how is code treated differently, and why?
%     \item Is code similar to natural language
%     \item If so how can we use machine translation techniques to help understanding of source code
%     \item How can we use nlp to help understanding of source code
%     \item How can we use source code to help understanding of source code with nlp
% \end{itemize}


Historically, academic research into software tooling has focused on formal and deductive methods. 
For instance, in the field of bug finding, static analysis has formed the basis of methods to reduce runtime errors of software, either through explicit checks \cite{okada_combination_2007}, or through reasoning about sets of execution traces
% by analysing syntax
\cite{bessey_few_2010}. 
This research exploits the fact that code has a structure that is logically consistent and can be reasoned about mathematically.
As a result, these deductive methods have been attractive due to their formal nature and the guarantees they can offer in the worst case.

 % and formally, despite ignoring potentially valuable non-semantic information such as variable name conventions, programming idioms or comments.  

However, the proliferation of online open-source code \cite{dyer_boa_2013} has provided opportunities for a more statistical approach. 
Instead of formalising rule-based systems, the development of tools can now be guided by the most statistically significant features of the code.
% CITE ALLAMANIS?
Such a transition, from rule-based approach to statistical approach, signals a different end-goal for these tools, abandoning worst case guarantees for better performance on average \cite{allamanis_survey_2017}. %most of the time.
% from a focus of best perfomance in the worst case (or in specific conditions), to best performance in most cases. PARAPHRASE
This promises to be of much value to growing computing industry. It also opens up tooling research to the wealth of statistical machine learning techniques.
 
This transition is fundamentally justifed under the Naturalness Hypothesis, proposed by Allamanis et al \cite{allamanis_survey_2017}. 
This hypothesis suggests that \textit{``Software is a form of human communications; software corpora have a similar statistical properties to natural language copora; and these properties can be exploited to build better software tools''}.\cite{allamanis_survey_2017} 
The hypothesis draws inspiration from Don Knuth's idea of \textit{literate programming}\cite{knuth_literate_1984}, that the primary task of programming should be ``explaining to human beings what we want a computer to do...''\cite{knuth_literate_1984}, not simply preparing a set of commands for a machine.

Allamanis cites the successes that recent statistical natural language processing approaches have had when applied to `Big Code', dating back to Hindle et al \cite{hindle_naturalness_nodate}, to defending his hypothesis. He also points to evidence in cognitive science studies, that demonstrate that brain behaviour in reading programming languages is akin to reading ``natural languages with greater expertise''\cite{floyd_decoding_2017}.

We mention the Naturalness Hypothesis because it is a fundamental inspiration for the approach and methods of our research.
We believe that it is only in looking at the statistical `naturalness' properties of both source code and the language accompanying it, that the generalisable, extenisble progress will be made in human-centric tasks such as code summarization, code explanation, and documentation generation.

%%% DIFFERENCES BETWEEN CODE & NL

\section{Problem Formulation} % (fold)
\label{sec:problem_formulation}

As the field of statistical research into code and its `naturalness' properties is relatively new, we feel there are opportunities to investigate the field of documentation generation, as it relates to specific elements of code. 
To this end, we examine the automatic generation of brief descriptions or summaries of Python function arguments, by examining their code, contextual data, and descriptions found in docstrings.

In particular we wish to investigate:
\begin{itemize}
    \item whether reasonable summaries can be generated from just lexical names in the function signature?
    \item whether reasonable summaries can be generated from the functions abstract syntax tree, without the lexical data?
    \item whether these models, can be combined in a way that surpasses each individually?
    \item whether such models can work both in an `in-project' setting and an `out-project setting'?
\end{itemize}
 
\section{Our Contributions}

Our contributions include a new and challenging dataset suitable for investigation in machine summarization of source code elements, comprised of 40,000 Python arguments and descriptions.
To the best of our knowledge, this is the first dataset suitable for the task of argument description generation, and the first study of such a problem.
We demonstrate the informative power of function signatures, using both Rote Learner models, and neural translation architectures.
We also present a new architecture capable of generating argument description solely from a syntax tree, and demonstrate its effectiveness in this task.
We demonstrate its ability to learn, despite partial and full occlusion of lexical data, and also present a multimodal archtitecture combining the lexical and syntactical code features.
We finally demonstrate the difficulty of applying such models between datasets, and show there is a still a significant challenge remaining on this dataset.


\section{Structure of the Thesis} % (fold)
\label{sec:structure_of_the_thesis}

In addressing the research questions posed above, we first present a brief survey of the existing literature for this field in Chapter 2, and a background of theory in Chapter 3.
Chapter 4 presents an in depth presentation of our new dataset, along with an analysis of its composition, while Chapter 5 presents the models which we used to investigate our central research question. 
Chapter 6 provides an overview of our method, while Chapter 7 presents  report of our experiments, their results, and their analysis.
Finally we present a conclusion of the results in Chapter 8, with an elaboration on potential future work

% section structure_of_the_thesis (end)


% section related_work (end)

 % subsection subsection_name (end) 


% Some stuff about things. \cite{example-citation} Some more things. 

% Inline citation: \bibentry{example-citation}

% This is just a bare misdnimum to get started.  There is unlimited guidance on using latex, e.g. {\tt https://en.wikibooks.org/wiki/LaTeX}.   You are still responsible to check the detailed requirements of a project, including formatting instructions, see \\
% {\tt https://moodle.ucl.ac.uk/pluginfile.php/3591429/mod\_resource/content/7/UGProjects2017.pdf}.
% Leave at least a line of white space when you want to start a new paragraph.

% Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, \sum_{i=0}^nf(i)$, or in display mode
% \[ e^{i\pi}=-1\] and another way, this time with labels,
% \begin{align}
% \label{line1} A=B\wedge B=C&\rightarrow A=C\\
% &\rightarrow C=A\\
% \intertext{note that}
% n!&=\prod_{1\leq i\leq n}i \\
% \int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
% \end{align}
% We can refer to labels like this \eqref{line1}. 

