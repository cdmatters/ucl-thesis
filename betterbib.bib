
@article{luong_effective_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.04025},
  primaryClass = {cs},
  title = {Effective {{Approaches}} to {{Attention}}-Based {{Neural Machine Translation}}},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  language = {en},
  journal = {arXiv:1508.04025 [cs]},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  month = aug,
  year = {2015},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/D9GSCUM8/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf},
  annote = {Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details}
}

@article{bahdanau_neural_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.0473},
  primaryClass = {cs, stat},
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder\textendash{}decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\textendash{}decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  language = {en},
  journal = {arXiv:1409.0473 [cs, stat]},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/4RSKGU6J/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf},
  annote = {Comment: Accepted at ICLR 2015 as oral presentation}
}

@article{kalchbrenner_recurrent_nodate,
  title = {Recurrent {{Continuous Translation Models}}},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is $>$ 43\% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  language = {en},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  pages = {10},
  file = {/Users/erichambro/Zotero/storage/GX3KHIIC/Kalchbrenner and Blunsom - Recurrent Continuous Translation Models.pdf}
}

@article{ghader_what_nodate,
  title = {What Does {{Attention}} in {{Neural Machine Translation Pay Attention}} To?},
  abstract = {Attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, attention is considered to be an alignment model as well. However, there is no work that specifically studies attention and provides analysis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.},
  language = {en},
  author = {Ghader, Hamidreza and Monz, Christof},
  pages = {10},
  file = {/Users/erichambro/Zotero/storage/KDL8ZRTH/Ghader and Monz - What does Attention in Neural Machine Translation .pdf}
}

@article{chaganty_price_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.02202},
  primaryClass = {cs},
  title = {The Price of Debiasing Automatic Metrics in Natural Language Evaluation},
  abstract = {For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7\textendash{}13\% cost reduction on evaluating summarization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks\textemdash{}the automatic metric and the prompt shown to human evaluators\textemdash{}both of which need to be improved to obtain greater cost savings.},
  language = {en},
  journal = {arXiv:1807.02202 [cs]},
  author = {Chaganty, Arun Tejasvi and Mussman, Stephen and Liang, Percy},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/VAEX8FQV/Chaganty et al. - 2018 - The price of debiasing automatic metrics in natura.pdf},
  annote = {Comment: To appear ACL 2018}
}

@article{movshovitz-attias_natural_nodate,
  title = {Natural {{Language Models}} for {{Predicting Programming Comments}}},
  abstract = {Statistical language models have successfully been used to describe and analyze natural language documents. Recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments. In this work, we predict comments from JAVA source files of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47\% of the comment typing.},
  language = {en},
  author = {{Movshovitz-Attias}, Dana and Cohen, William W},
  pages = {6},
  file = {/Users/erichambro/Zotero/storage/NMS92QHI/Movshovitz-Attias and Cohen - Natural Language Models for Predicting Programming.pdf}
}

@inproceedings{nguyen_statistical_2013,
  address = {Saint Petersburg, Russia},
  title = {A Statistical Semantic Language Model for Source Code},
  isbn = {978-1-4503-2237-9},
  doi = {10.1145/2491411.2491458},
  abstract = {Recent research has successfully applied the statistical ngram language model to show that source code exhibits a good level of repetition. The n-gram model is shown to have good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to capture source code regularities/patterns is based only on the lexical information in a local context of the code units. To improve predictability, we introduce SLAMC, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/ functionality into an n-gram topic model, together with pairwise associations of program elements. Based on SLAMC, we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18\textendash{}68\% higher accuracy than the state-of-the-art approach.},
  language = {en},
  booktitle = {Proceedings of the 2013 9th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}} - {{ESEC}}/{{FSE}} 2013},
  publisher = {{ACM Press}},
  author = {Nguyen, Tung Thanh and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
  year = {2013},
  pages = {532},
  file = {/Users/erichambro/Zotero/storage/4SDSUGCE/Nguyen et al. - 2013 - A statistical semantic language model for source c.pdf}
}

@inproceedings{karaivanov_phrase-based_2014,
  address = {Portland, Oregon, USA},
  title = {Phrase-{{Based Statistical Translation}} of {{Programming Languages}}},
  isbn = {978-1-4503-3210-1},
  doi = {10.1145/2661136.2661148},
  abstract = {Phrase-based statistical machine translation approaches have been highly successful in translating between natural languages and are heavily used by commercial systems (e.g. Google Translate).},
  language = {en},
  booktitle = {Proceedings of the 2014 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} \& {{Software}} - {{Onward}}! '14},
  publisher = {{ACM Press}},
  author = {Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin},
  year = {2014},
  pages = {173-184},
  file = {/Users/erichambro/Zotero/storage/479BZVYI/Karaivanov et al. - 2014 - Phrase-Based Statistical Translation of Programmin.pdf}
}

@inproceedings{iyer_summarizing_2016,
  address = {Berlin, Germany},
  title = {Summarizing {{Source Code}} Using a {{Neural Attention Model}}},
  doi = {10.18653/v1/P16-1195},
  abstract = {High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely datadriven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C\# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C\# benchmark by a large margin.},
  language = {en},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  author = {Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
  year = {2016},
  pages = {2073-2083},
  file = {/Users/erichambro/Zotero/storage/C49SKRPS/Iyer et al. - 2016 - Summarizing Source Code using a Neural Attention M.pdf}
}

@article{allamanis_convolutional_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.03001},
  primaryClass = {cs},
  title = {A {{Convolutional Attention Network}} for {{Extreme Summarization}} of {{Source Code}}},
  abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network's performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
  language = {en},
  journal = {arXiv:1602.03001 [cs]},
  author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/erichambro/Zotero/storage/7GIUG67L/Allamanis et al. - 2016 - A Convolutional Attention Network for Extreme Summ.pdf},
  annote = {Comment: Code, data and visualization at http://groups.inf.ed.ac.uk/cup/codeattention/}
}

@inproceedings{raychev_predicting_2015,
  address = {Mumbai, India},
  title = {Predicting {{Program Properties}} from "{{Big Code}}"},
  isbn = {978-1-4503-3300-9},
  doi = {10.1145/2676726.2677009},
  abstract = {We present a new approach for predicting program properties from massive codebases (aka ``Big Code''). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs.},
  language = {en},
  booktitle = {Proceedings of the 42nd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}} - {{POPL}} '15},
  publisher = {{ACM Press}},
  author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
  year = {2015},
  pages = {111-124},
  file = {/Users/erichambro/Zotero/storage/Q3NAJQRM/Raychev et al. - 2015 - Predicting Program Properties from Big Code.pdf}
}

@article{allamanis_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00740},
  primaryClass = {cs},
  title = {Learning to {{Represent Programs}} with {{Graphs}}},
  abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.},
  language = {en},
  journal = {arXiv:1711.00740 [cs]},
  author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/SDJUV5PZ/Allamanis et al. - 2017 - Learning to Represent Programs with Graphs.pdf},
  annote = {Comment: Published in ICLR 2018. arXiv admin note: text overlap with arXiv:1705.07867}
}

@article{bhoopchand_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.08307},
  primaryClass = {cs},
  title = {Learning {{Python Code Suggestion}} with a {{Sparse Pointer Network}}},
  abstract = {To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very longrange dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.},
  language = {en},
  journal = {arXiv:1611.08307 [cs]},
  author = {Bhoopchand, Avishkar and Rockt\"aschel, Tim and Barr, Earl and Riedel, Sebastian},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Computer Science - Software Engineering,Computer Science - Artificial Intelligence},
  file = {/Users/erichambro/Zotero/storage/BS9NQECI/Bhoopchand et al. - 2016 - Learning Python Code Suggestion with a Sparse Poin.pdf},
  annote = {Comment: Under review as a conference paper at ICLR 2017}
}

@article{allamanis_mining_nodate,
  title = {Mining {{Semantic Loop Idioms}} from {{Big Code}}},
  abstract = {During maintenance, developers spend a lot of time transforming existing code: refactoring, optimizing, and adding checks to make it more robust. Much of this work is the drudgery of identifying and replacing specific patterns, yet it resists automation, because of meaningful patterns are hard to automatically find. We present a technique for mining loop idioms, surprisingly probable semantic patterns that occur in loops, from big code to find meaningful patterns. First, we show that automatically identifiable patterns exist, in great numbers, with a large scale empirical study of loop over 25 MLOC. We find that loops in this corpus are simple and predictable: 90\% of them have fewer than 15LOC and 90\% have no nesting and very simple control structure. Encouraged by this result, we coil loops to abstract away syntactic diversity to define information rich loop idioms. We show that only 50 loop idioms cover 50\% of the concrete loops. We show how loop idioms can help a tool developers identify and prioritize refactorings. We also show how our framework opens the door to data-driven tool and language design discovering opportunities to introduce new API calls and language constructs: loop idioms show that LINQ would benefit from an Enumerate operator, a result confirmed by the fact that precisely this feature is one of the most requested features on StackOverflow with 197 votes and 95k views.},
  language = {en},
  author = {Allamanis, Miltiadis and Devanbu, Premkumar and Barr, Earl T and Marron, Mark},
  pages = {20},
  file = {/Users/erichambro/Zotero/storage/9DT5T9XR/Allamanis et al. - Mining Semantic Loop Idioms from Big Code.pdf}
}

@article{alon_code2vec_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09473},
  primaryClass = {cs, stat},
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  shorttitle = {Code2vec},
  abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent code as a collection of paths in its abstract syntax tree, and aggregate these paths, in a smart and scalable way, into a single fixed-length code vector, which can be used to predict semantic properties of the snippet. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75\%, being the first to successfully predict method names based on a large, cross-project, corpus.},
  language = {en},
  journal = {arXiv:1803.09473 [cs, stat]},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/TZWRQSP6/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf}
}

@article{alon_code2seq_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.01400},
  primaryClass = {cs, stat},
  title = {Code2seq: {{Generating Sequences}} from {{Structured Representations}} of {{Code}}},
  shorttitle = {Code2seq},
  abstract = {The ability to generate natural language sequences from source code snippets can be used for code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of paths in its abstract syntax tree (AST) and uses attention to select the relevant paths during decoding, much like contemporary NMT models. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.},
  language = {en},
  journal = {arXiv:1808.01400 [cs, stat]},
  author = {Alon, Uri and Levy, Omer and Yahav, Eran},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/UMSJDARJ/Alon et al. - 2018 - code2seq Generating Sequences from Structured Rep.pdf}
}

@inproceedings{shi_why_2016,
  address = {Austin, Texas},
  title = {Why {{Neural Translations}} Are the {{Right Length}}},
  doi = {10.18653/v1/D16-1248},
  abstract = {We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality.},
  language = {en},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Shi, Xing and Knight, Kevin and Yuret, Deniz},
  year = {2016},
  pages = {2278-2282},
  file = {/Users/erichambro/Zotero/storage/FM7HE4DZ/Shi et al. - 2016 - Why Neural Translations are the Right Length.pdf}
}

@article{noraset_definition_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.00394},
  primaryClass = {cs},
  title = {Definition {{Modeling}}: {{Learning}} to Define Word Embeddings in Natural Language},
  shorttitle = {Definition {{Modeling}}},
  abstract = {Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a characterlevel convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.},
  language = {en},
  journal = {arXiv:1612.00394 [cs]},
  author = {Noraset, Thanapon and Liang, Chen and Birnbaum, Larry and Downey, Doug},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/PUYZ3SVR/Noraset et al. - 2016 - Definition Modeling Learning to define word embed.pdf},
  annote = {Comment: To appear in AAAI Conference 2017}
}

@article{alon_code2vec_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09473},
  primaryClass = {cs, stat},
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  shorttitle = {Code2vec},
  abstract = {We present a neural model for representing snippets of code as continuous distributed vectors. The main idea is to represent code as a collection of paths in its abstract syntax tree, and aggregate these paths, in a smart and scalable way, into a single fixed-length \$$\backslash$textit\{code vector\}\$, which can be used to predict semantic properties of the snippet. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of \$14\$M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over \$75$\backslash$\%\$, being the first to successfully predict method names based on a large, cross-project, corpus.},
  journal = {arXiv:1803.09473 [cs, stat]},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/2I3B96SA/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf;/Users/erichambro/Zotero/storage/V3FAY7TY/1803.html}
}

@article{alon_code2vec_2018-2,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09473},
  primaryClass = {cs, stat},
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  shorttitle = {Code2vec},
  abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent code as a collection of paths in its abstract syntax tree, and aggregate these paths, in a smart and scalable way, into a single fixed-length code vector, which can be used to predict semantic properties of the snippet. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75\%, being the first to successfully predict method names based on a large, cross-project, corpus.},
  language = {en},
  journal = {arXiv:1803.09473 [cs, stat]},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/5QZMXH8H/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf}
}

@article{alon_code2seq_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.01400},
  primaryClass = {cs, stat},
  title = {Code2seq: {{Generating Sequences}} from {{Structured Representations}} of {{Code}}},
  shorttitle = {Code2seq},
  abstract = {The ability to generate natural language sequences from source code snippets can be used for code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of paths in its abstract syntax tree (AST) and uses attention to select the relevant paths during decoding, much like contemporary NMT models. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.},
  language = {en},
  journal = {arXiv:1808.01400 [cs, stat]},
  author = {Alon, Uri and Levy, Omer and Yahav, Eran},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/SQTHPGG7/Alon et al. - 2018 - code2seq Generating Sequences from Structured Rep.pdf}
}

@article{oda_learning_nodate,
  title = {Learning to {{Generate Pseudo}}-Code from {{Source Code}} Using {{Statistical Machine Translation}}},
  abstract = {Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.},
  language = {en},
  author = {Oda, Yusuke and Fudaba, Hiroyuki and Neubig, Graham and Hata, Hideaki and Sakti, Sakriani and Toda, Tomoki and Nakamura, Satoshi},
  pages = {11},
  file = {/Users/erichambro/Zotero/storage/BVJHLFET/Oda et al. - Learning to Generate Pseudo-code from Source Code .pdf}
}

@article{barzilay_using_nodate,
  title = {Using {{Semantic Unification}} to {{Generate Regular Expressions}} from {{Natural Language}}},
  language = {en},
  author = {Barzilay, Nate Kushman Regina},
  pages = {11},
  file = {/Users/erichambro/Zotero/storage/LFXAUCGB/Barzilay - Using Semantic Uniﬁcation to Generate Regular Expr.pdf}
}

@article{bessey_few_2010,
  title = {A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs in the Real World},
  volume = {53},
  issn = {00010782},
  shorttitle = {A Few Billion Lines of Code Later},
  doi = {10.1145/1646353.1646374},
  language = {en},
  number = {2},
  journal = {Communications of the ACM},
  author = {Bessey, Al and Engler, Dawson and Block, Ken and Chelf, Ben and Chou, Andy and Fulton, Bryan and Hallem, Seth and {Henri-Gros}, Charles and Kamsky, Asya and McPeak, Scott},
  month = feb,
  year = {2010},
  pages = {66-75},
  file = {/Users/erichambro/Zotero/storage/9YK4X6SC/Bessey et al. - 2010 - A few billion lines of code later using static an.pdf}
}

@incollection{okada_combination_2007,
  address = {Berlin, Heidelberg},
  title = {Combination of {{Abstractions}} in the {{ASTR\'EE Static Analyzer}}},
  volume = {4435},
  isbn = {978-3-540-77504-1 978-3-540-77505-8},
  abstract = {We describe the structure of the abstract domains in the Astre\textasciiacute{}e static analyzer, their modular organization into a hierarchical network, their cooperation to over-approximate the conjunction/reduced product of different abstractions and to ensure termination using collaborative widenings and narrowings. This separation of the abstraction into a combination of cooperative abstract domains makes Astre\textasciiacute{}e extensible, an essential feature to cope with false alarms and ultimately provide sound formal verification of the absence of runtime errors in very large software.},
  language = {en},
  booktitle = {Advances in {{Computer Science}} - {{ASIAN}} 2006. {{Secure Software}} and {{Related Issues}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Cousot, Patrick and Cousot, Radhia and Feret, J\'er\^ome and Mauborgne, Laurent and Min\'e, Antoine and Monniaux, David and Rival, Xavier},
  editor = {Okada, Mitsu and Satoh, Ichiro},
  year = {2007},
  pages = {272-300},
  file = {/Users/erichambro/Zotero/storage/N629XJM4/Cousot et al. - 2007 - Combination of Abstractions in the ASTRÉE Static A.pdf},
  doi = {10.1007/978-3-540-77505-8_23}
}

@misc{knuth_donald_e._knuthweb.pdf_nodate,
  title = {Knuthweb.Pdf},
  howpublished = {http://dx.doi.org/10.1093/comjnl/27.2.97},
  author = {Knuth, Donald E.},
  file = {/Users/erichambro/Zotero/storage/GU3M6LBI/knuthweb.pdf}
}

@article{knuth_literate_1984,
  title = {Literate {{Programming}}},
  volume = {27},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
  number = {2},
  journal = {The Computer Journal},
  author = {Knuth, D. E.},
  month = jan,
  year = {1984},
  pages = {97-111},
  annote = {10.1093/comjnl/27.2.97}
}

@inproceedings{floyd_decoding_2017,
  title = {Decoding the {{Representation}} of {{Code}} in the {{Brain}}: {{An fMRI Study}} of {{Code Review}} and {{Expertise}}},
  shorttitle = {Decoding the {{Representation}} of {{Code}} in the {{Brain}}},
  doi = {10.1109/ICSE.2017.24},
  abstract = {Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79\%, p $<$; 0.001). Further, we find that the same set of brain regions distinguish between code and prose (near-perfect correlation, r = 0.99, p $<$; 0.001). Finally, we find that task distinctions are modulated by expertise, such that greater skill predicts a less differentiated neural representation (r = -0.44, p = 0.016) indicating that more skilled participants treat code and prose more similarly at a neural activation level.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Floyd, B. and Santander, T. and Weimer, W.},
  month = may,
  year = {2017},
  keywords = {Biomedical imaging,biomedical MRI,Brain,code comprehension,code review,Computer science,fMRI study,functional magnetic resonance imaging,medical image processing,medical imaging,medical imaging techniques,natural languages,Natural languages,neural representations,programming languages,prose review,Software,software engineering,Software engineering,software engineering tasks,Tools},
  pages = {175-186},
  file = {/Users/erichambro/Zotero/storage/RGY4LN3T/Floyd et al. - 2017 - Decoding the Representation of Code in the Brain .pdf;/Users/erichambro/Zotero/storage/3ZUDX2UK/7985660.html}
}

@article{hindle_naturalness_nodate,
  title = {On the Naturalness of Software},
  abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations\textemdash{}and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.},
  language = {en},
  author = {Hindle, Abram and Barr, Earl T and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
  pages = {11},
  file = {/Users/erichambro/Zotero/storage/E85YXJAP/Hindle et al. - On the naturalness of software.pdf}
}

@inproceedings{sridhara_[not_2010,
  address = {Antwerp, Belgium},
  title = {[{{NOT NATURALNESS}}] {{Towards}} Automatically Generating Summary Comments for {{Java}} Methods},
  isbn = {978-1-4503-0116-9},
  doi = {10.1145/1858996.1859006},
  language = {en},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} International Conference on {{Automated}} Software Engineering - {{ASE}} '10},
  publisher = {{ACM Press}},
  author = {Sridhara, Giriprasad and Hill, Emily and Muppaneni, Divya and Pollock, Lori and {Vijay-Shanker}, K.},
  year = {2010},
  pages = {43},
  file = {/Users/erichambro/Zotero/storage/5LNWWVKR/Sridhara et al. - 2010 - Towards automatically generating summary comments .pdf}
}

@inproceedings{sridhara_[not_2011,
  address = {Waikiki, Honolulu, HI, USA},
  title = {[{{NOT NATURALNESS}}] {{Automatically}} Detecting and Describing High Level Actions within Methods},
  isbn = {978-1-4503-0445-0},
  doi = {10.1145/1985793.1985808},
  abstract = {One approach to easing program comprehension is to reduce the amount of code that a developer has to read. Describing the high level abstract algorithmic actions associated with code fragments using succinct natural language phrases potentially enables a newcomer to focus on fewer and more abstract concepts when trying to understand a given method. Unfortunately, such descriptions are typically missing because it is tedious to create them manually.},
  language = {en},
  booktitle = {Proceeding of the 33rd International Conference on {{Software}} Engineering - {{ICSE}} '11},
  publisher = {{ACM Press}},
  author = {Sridhara, Giriprasad and Pollock, Lori and {Vijay-Shanker}, K.},
  year = {2011},
  pages = {101},
  file = {/Users/erichambro/Zotero/storage/MZ5HUG46/Sridhara et al. - 2011 - Automatically detecting and describing high level .pdf}
}

@inproceedings{buse_[not_2010,
  address = {Antwerp, Belgium},
  title = {[{{NOT NATURALNESS}}] {{Automatically}} Documenting Program Changes},
  isbn = {978-1-4503-0116-9},
  doi = {10.1145/1858996.1859005},
  abstract = {Source code modifications are often documented with log messages. Such messages are a key component of software maintenance: they can help developers validate changes, locate and triage defects, and understand modifications. However, this documentation can be burdensome to create and can be incomplete or inaccurate.},
  language = {en},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} International Conference on {{Automated}} Software Engineering - {{ASE}} '10},
  publisher = {{ACM Press}},
  author = {Buse, Raymond P.L. and Weimer, Westley R.},
  year = {2010},
  pages = {33},
  file = {/Users/erichambro/Zotero/storage/V5UMSCFF/Buse and Weimer - 2010 - Automatically documenting program changes.pdf},
  annote = {AUTOMATIC PROGRAM ANALYSIS not BIG CODE}
}

@inproceedings{gabel_study_2010,
  address = {Santa Fe, New Mexico, USA},
  title = {A Study of the Uniqueness of Source Code},
  isbn = {978-1-60558-791-2},
  doi = {10.1145/1882291.1882315},
  abstract = {This paper presents the results of the first study of the uniqueness of source code. We define the uniqueness of a unit of source code with respect to the entire body of written software, which we approximate with a corpus of 420 million lines of source code. Our high-level methodology consists of examining a collection of 6,000 software projects and measuring the degree to which each project can be `assembled' solely from portions of this corpus, thus providing a precise measure of `uniqueness' that we call syntactic redundancy. We parameterized our study over a variety of variables, the most important of which being the level of granularity at which we view source code. Our suite of experiments together consumed approximately four months of CPU time, providing quantitative answers to the following questions: at what levels of granularity is software unique, and at a given level of granularity, how unique is software? While we believe these questions to be of intrinsic interest, we discuss possible applications to genetic programming and developer productivity tools.},
  language = {en},
  booktitle = {Proceedings of the Eighteenth {{ACM SIGSOFT}} International Symposium on {{Foundations}} of Software Engineering - {{FSE}} '10},
  publisher = {{ACM Press}},
  author = {Gabel, Mark and Su, Zhendong},
  year = {2010},
  pages = {147},
  file = {/Users/erichambro/Zotero/storage/JUMBYXZP/Gabel and Su - 2010 - A study of the uniqueness of source code.pdf}
}

@article{tu_localness_nodate,
  title = {On the {{Localness}} of {{Software}}},
  abstract = {The n-gram language model, which has its roots in statistical natural language processing, has been shown to successfully capture the repetitive and predictable regularities (``naturalness") of source code, and help with tasks such as code suggestion, porting, and designing assistive coding devices. However, we show in this paper that this natural-language-based model fails to exploit a special property of source code: localness. We find that human-written programs are localized: they have useful local regularities that can be captured and exploited. We introduce a novel cache language model that consists of both an n-gram and an added ``cache" component to exploit localness. We show empirically that the additional cache component greatly improves the n-gram approach by capturing the localness of software, as measured by both cross-entropy and suggestion accuracy. Our model's suggestion accuracy is actually comparable to a state-of-the-art, semantically augmented language model; but it is simpler and easier to implement. Our cache language model requires nothing beyond lexicalization, and thus is applicable to all programming languages.},
  language = {en},
  author = {Tu, Zhaopeng and Su, Zhendong and Devanbu, Premkumar},
  pages = {12},
  file = {/Users/erichambro/Zotero/storage/VREW3WCB/Tu et al. - On the Localness of Software.pdf}
}

@article{loyola_neural_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04856},
  primaryClass = {cs},
  title = {A {{Neural Architecture}} for {{Generating Natural Language Descriptions}} from {{Source Code Changes}}},
  abstract = {We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.},
  language = {en},
  journal = {arXiv:1704.04856 [cs]},
  author = {Loyola, Pablo and {Marrese-Taylor}, Edison and Matsuo, Yutaka},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/GIPBWBUC/Loyola et al. - 2017 - A Neural Architecture for Generating Natural Langu.pdf},
  annote = {Comment: Accepted at ACL 2017}
}

@inproceedings{white_toward_2015,
  address = {Florence, Italy},
  title = {Toward {{Deep Learning Software Repositories}}},
  isbn = {978-0-7695-5594-2},
  doi = {10.1109/MSR.2015.38},
  abstract = {Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cachebased n-grams on a corpus of Java projects. We experiment with two of the models' hyperparameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.},
  language = {en},
  booktitle = {2015 {{IEEE}}/{{ACM}} 12th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{IEEE}},
  author = {White, Martin and Vendome, Christopher and {Linares-Vasquez}, Mario and Poshyvanyk, Denys},
  month = may,
  year = {2015},
  pages = {334-345},
  file = {/Users/erichambro/Zotero/storage/C63RPBHH/White et al. - 2015 - Toward Deep Learning Software Repositories.pdf}
}

@article{allamanis_survey_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.06182},
  primaryClass = {cs},
  title = {A {{Survey}} of {{Machine Learning}} for {{Big Code}} and {{Naturalness}}},
  abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit code's abundance of patterns. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
  language = {en},
  journal = {arXiv:1709.06182 [cs]},
  author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/XBID69AZ/Allamanis et al. - 2017 - A Survey of Machine Learning for Big Code and Natu.pdf},
  annote = {Comment: Website accompanying this survey paper can be found at https://ml4code.github.io}
}

@inproceedings{allamanis_mining_2013,
  address = {San Francisco, CA, USA},
  title = {Mining Source Code Repositories at Massive Scale Using Language Modeling},
  isbn = {978-1-4673-2936-1 978-1-4799-0345-0},
  doi = {10.1109/MSR.2013.6624029},
  abstract = {The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new ``lens'' for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program's core logic based solely on general information theoretic criteria.},
  language = {en},
  booktitle = {2013 10th {{Working Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  publisher = {{IEEE}},
  author = {Allamanis, Miltiadis and Sutton, Charles},
  month = may,
  year = {2013},
  pages = {207-216},
  file = {/Users/erichambro/Zotero/storage/5D32EAMW/Allamanis and Sutton - 2013 - Mining source code repositories at massive scale u.pdf}
}

@article{barone_parallel_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.02275},
  primaryClass = {cs},
  title = {A Parallel Corpus of {{Python}} Functions and Documentation Strings for Automated Code Documentation and Code Generation},
  abstract = {Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains.},
  language = {en},
  journal = {arXiv:1707.02275 [cs]},
  author = {Barone, Antonio Valerio Miceli and Sennrich, Rico},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  file = {/Users/erichambro/Zotero/storage/F6XMRWZG/Barone and Sennrich - 2017 - A parallel corpus of Python functions and document.pdf},
  annote = {Comment: 5 pages, 1 figure, 3 tables}
}

@article{hochreiter_long_1997,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  number = {8},
  journal = {Neural Computation},
  author = {Hochreiter, Sepp and Schmidhuber, J\"urgen},
  month = nov,
  year = {1997},
  pages = {1735-1780},
  annote = {doi: 10.1162/neco.1997.9.8.1735}
}

@misc{noauthor_numpydoc_nodate,
  title = {Numpydoc Docstring Guide \textemdash{} Numpydoc v0.9.Dev0 {{Manual}}},
  howpublished = {https://numpydoc.readthedocs.io/en/latest/format.html\#import-conventions},
  file = {/Users/erichambro/Zotero/storage/R55NST9C/format.html}
}

@misc{noauthor_style_2018,
  title = {Style Guides for {{Google}}-Originated Open-Source Projects: Google/Styleguide},
  shorttitle = {Style Guides for {{Google}}-Originated Open-Source Projects},
  organization = {{Google}},
  month = aug,
  year = {2018}
}

@inproceedings{dyer_boa_2013,
  address = {San Francisco, CA, USA},
  title = {Boa: {{A}} Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories},
  isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
  shorttitle = {Boa},
  doi = {10.1109/ICSE.2013.6606588},
  abstract = {In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.},
  language = {en},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  publisher = {{IEEE}},
  author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
  month = may,
  year = {2013},
  pages = {422-431},
  file = {/Users/erichambro/Zotero/storage/JVJ4LBPM/Dyer et al. - 2013 - Boa A language and infrastructure for analyzing u.pdf}
}

@misc{noauthor_understanding_nodate,
  title = {Understanding {{LSTM Networks}} -- Colah's Blog},
  howpublished = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  file = {/Users/erichambro/Zotero/storage/GR65W47V/2015-08-Understanding-LSTMs.html}
}

@book{Goodfellow:2016:DL:3086952,
  title = {Deep {{Learning}}},
  isbn = {0-262-03561-8 978-0-262-03561-3},
  publisher = {{The MIT Press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016}
}

@article{nielsenneural,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2018},
  keywords = {Neural and book deep learning networks},
  type = {misc},
  publisher = {{Determination Press}},
  biburl = {https://www.bibsonomy.org/bibtex/274383acee84241145ff4ffede9658206/martin29},
  interhash = {04d527cadd39f888fc3babcad3343362},
  intrahash = {74383acee84241145ff4ffede9658206}
}

@article{bengio_neural_nodate,
  title = {A {{Neural Probabilistic Language Model}}},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  language = {en},
  author = {Bengio, Yoshua and Ducharme, R\'ejean and Vincent, Pascal and Jauvin, Christian},
  pages = {19},
  file = {/Users/erichambro/Zotero/storage/UTZ77SZZ/Bengio et al. - A Neural Probabilistic Language Model.pdf}
}

@article{chung_empirical_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.3555},
  primaryClass = {cs},
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  language = {en},
  journal = {arXiv:1412.3555 [cs]},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/erichambro/Zotero/storage/NH46RA5B/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf},
  annote = {Comment: Presented in NIPS 2014 Deep Learning and Representation Learning Workshop}
}

@article{cho_properties_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.1259},
  primaryClass = {cs, stat},
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}-{{Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder\textendash{}Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  language = {en},
  journal = {arXiv:1409.1259 [cs, stat]},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/2XASLWJ9/Cho et al. - 2014 - On the Properties of Neural Machine Translation E.pdf},
  annote = {Comment: Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8)}
}

@article{lipton_critical_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.00019},
  primaryClass = {cs},
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a selfcontained explication of the state of the art together with a historical perspective and references to primary research.},
  language = {en},
  journal = {arXiv:1506.00019 [cs]},
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  month = may,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/erichambro/Zotero/storage/ECQAQQRC/Lipton et al. - 2015 - A Critical Review of Recurrent Neural Networks for.pdf}
}

@article{bengio_learning_1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  volume = {5},
  issn = {1045-9227},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.$<$$>$},
  number = {2},
  journal = {IEEE Transactions on Neural Networks},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  month = mar,
  year = {1994},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,efficient learning,gradient descent,input/output sequence mapping,Intelligent networks,learning (artificial intelligence),long-term dependencies,Neural networks,Neurofeedback,numerical analysis,prediction problems,Production,production problems,recognition,recurrent neural nets,recurrent neural network training,Recurrent neural networks,temporal contingencies},
  pages = {157-166},
  file = {/Users/erichambro/Zotero/storage/GHTPCLRQ/Bengio et al. - 1994 - Learning long-term dependencies with gradient desc.pdf;/Users/erichambro/Zotero/storage/STM5JKRU/279181.html}
}

@article{melis_state_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.05589},
  primaryClass = {cs},
  title = {On the {{State}} of the {{Art}} of {{Evaluation}} in {{Neural Language Models}}},
  abstract = {Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing codebases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.},
  language = {en},
  journal = {arXiv:1707.05589 [cs]},
  author = {Melis, G\'abor and Dyer, Chris and Blunsom, Phil},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/R3R3KSMW/Melis et al. - 2017 - On the State of the Art of Evaluation in Neural La.pdf}
}

@article{sutskever_sequence_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.3215},
  primaryClass = {cs},
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  language = {en},
  journal = {arXiv:1409.3215 [cs]},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/C7ZJS2U5/Sutskever et al. - 2014 - Sequence to Sequence Learning with Neural Networks.pdf},
  annote = {Comment: 9 pages}
}

@article{kalchbrenner_recurrent_nodate-1,
  title = {Recurrent {{Continuous Translation Models}}},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is $>$ 43\% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  language = {en},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  pages = {10},
  file = {/Users/erichambro/Zotero/storage/NCHWNUXX/Kalchbrenner and Blunsom - Recurrent Continuous Translation Models.pdf}
}

@article{luong_effective_2015-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.04025},
  primaryClass = {cs},
  title = {Effective {{Approaches}} to {{Attention}}-Based {{Neural Machine Translation}}},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  language = {en},
  journal = {arXiv:1508.04025 [cs]},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  month = aug,
  year = {2015},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/VIDNQTLI/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf},
  annote = {Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details}
}

@article{mikolov_efficient_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  language = {en},
  journal = {arXiv:1301.3781 [cs]},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/WERWK3CV/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf}
}

@inproceedings{pennington_glove_2014,
  address = {Doha, Qatar},
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  doi = {10.3115/v1/D14-1162},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  language = {en},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  publisher = {{Association for Computational Linguistics}},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532-1543},
  file = {/Users/erichambro/Zotero/storage/PXLKQYKZ/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf}
}

@article{mikolov_distributed_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1310.4546},
  primaryClass = {cs, stat},
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
  language = {en},
  journal = {arXiv:1310.4546 [cs, stat]},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month = oct,
  year = {2013},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/D2RHZL8G/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@article{mikolov_efficient_2013-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1301.3781},
  primaryClass = {cs},
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  language = {en},
  journal = {arXiv:1301.3781 [cs]},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  month = jan,
  year = {2013},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/SE38VMAH/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf}
}

@article{vaswani_attention_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03762},
  primaryClass = {cs},
  title = {Attention {{Is All You Need}}},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  language = {en},
  journal = {arXiv:1706.03762 [cs]},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  month = jun,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/BFKXJSXJ/Vaswani et al. - 2017 - Attention Is All You Need.pdf},
  annote = {Comment: 15 pages, 5 figures}
}

@article{hinton_distributed_nodate,
  title = {Distributed Representations},
  abstract = {Given a \^network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements.},
  language = {en},
  author = {Hinton, Geoffrey E},
  pages = {35},
  file = {/Users/erichambro/Zotero/storage/4K5QLK2B/Hinton - Distributed representations.pdf}
}

@article{xu_show_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03044},
  primaryClass = {cs},
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  language = {en},
  journal = {arXiv:1502.03044 [cs]},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/erichambro/Zotero/storage/RFFQMZJB/Xu et al. - 2015 - Show, Attend and Tell Neural Image Caption Genera.pdf}
}

@article{harris_distributional_1954,
  title = {Distributional {{Structure}}},
  volume = {10},
  issn = {0043-7956, 2373-5112},
  doi = {10.1080/00437956.1954.11659520},
  language = {en},
  number = {2-3},
  journal = {\emph{WORD}},
  author = {Harris, Zellig S.},
  month = aug,
  year = {1954},
  pages = {146-162},
  file = {/Users/erichambro/Zotero/storage/G7ZGL5MP/Harris - 1954 - Distributional Structure.pdf}
}

@book{rupert_firth_technique_2008,
  title = {The Technique of Semantics},
  volume = {34},
  author = {Rupert Firth, John},
  month = mar,
  year = {2008},
  doi = {10.1111/j.1467-968X.1935.tb01254.x}
}

@article{firth_technique_1935,
  title = {The {{Technique}} of {{Semantics}}.},
  volume = {34},
  issn = {1467-968X},
  doi = {10.1111/j.1467-968X.1935.tb01254.x},
  language = {en},
  number = {1},
  journal = {Transactions of the Philological Society},
  author = {Firth, J. R.},
  month = nov,
  year = {1935},
  pages = {36-73},
  file = {/Users/erichambro/Zotero/storage/UI59K8UR/Firth - 1935 - The Technique of Semantics..pdf;/Users/erichambro/Zotero/storage/27227GUF/j.1467-968X.1935.tb01254.html}
}

@incollection{Firth1957,
  booktitle = {Studies in {{Linguistic Analysis}}},
  author = {Firth, J.},
  biburl = {https://www.bibsonomy.org/bibtex/20b627387b63b652898cb5ecf03f87356/evabl444},
  description = {My thesis references}
}

@incollection{Firth1957,
  title = {A {{Synopsis}} of {{Linguistic Theory}} 1930-1955},
  booktitle = {Studies in {{Linguistic Analysis}}},
  publisher = {{Philological Society, Oxford}},
  author = {Firth, J.},
  year = {1957},
  keywords = {imported},
  biburl = {https://www.bibsonomy.org/bibtex/20b627387b63b652898cb5ecf03f87356/evabl444},
  description = {My thesis references},
  interhash = {49b3d847512a3bff2a1d77cd5ea5391f},
  intrahash = {0b627387b63b652898cb5ecf03f87356},
  owner = {BlEv},
  note = {reprinted in Palmer, F. (ed. 1968) Selected Papers of J. R. Firth, Longman, Harlow.}
}

@article{deerwester_indexing_1990,
  title = {Indexing by Latent Semantic Analysis},
  volume = {41},
  issn = {0002-8231, 1097-4571},
  doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
  abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.},
  language = {en},
  number = {6},
  journal = {Journal of the American Society for Information Science},
  author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
  month = sep,
  year = {1990},
  pages = {391-407},
  file = {/Users/erichambro/Zotero/storage/63QZM6DI/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf}
}

@article{alon_general_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09544},
  primaryClass = {cs},
  title = {A {{General Path}}-{{Based Representation}} for {{Predicting Program Properties}}},
  abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree (AST). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens.},
  language = {en},
  journal = {arXiv:1803.09544 [cs]},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/49L8SQTE/Alon et al. - 2018 - A General Path-Based Representation for Predicting.pdf},
  annote = {Comment: to appear in PLDI 2018}
}

@article{raychev_probabilistic_nodate,
  title = {Probabilistic {{Model}} for {{Code}} with {{Decision Trees}}},
  abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).},
  language = {en},
  author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
  pages = {17},
  file = {/Users/erichambro/Zotero/storage/GJZQ3BQH/Raychev et al. - Probabilistic Model for Code with Decision Trees.pdf}
}

@article{maddison_structured_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.0514},
  primaryClass = {cs, stat},
  title = {Structured {{Generative Models}} of {{Natural Source Code}}},
  abstract = {We study the problem of building generative models of natural source code (NSC); that is, source code written by humans and meant to be understood by humans. Our primary contribution is to describe new generative models that are tailored to NSC. The models are based on probabilistic context free grammars (PCFGs) and neuro-probabilistic language models (Mnih \& Teh, 2012), which are extended to incorporate additional source code-specific structure. These models can be efficiently trained on a corpus of source code and outperform a variety of less structured baselines in terms of predictive log likelihoods on held-out data.},
  language = {en},
  journal = {arXiv:1401.0514 [cs, stat]},
  author = {Maddison, Chris J. and Tarlow, Daniel},
  month = jan,
  year = {2014},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/SQ6QRPQK/Maddison and Tarlow - 2014 - Structured Generative Models of Natural Source Cod.pdf}
}

@article{kingma2014adam,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  journal = {arXiv preprint arXiv:1412.6980},
  author = {Kingma, Diederik P and Ba, Jimmy},
  year = {2014}
}

@article{ruder_overview_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04747},
  primaryClass = {cs},
  title = {An Overview of Gradient Descent Optimization Algorithms},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  language = {en},
  journal = {arXiv:1609.04747 [cs]},
  author = {Ruder, Sebastian},
  month = sep,
  year = {2016},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/L948ZGE5/Ruder - 2016 - An overview of gradient descent optimization algor.pdf},
  annote = {Comment: Added derivations of AdaMax and Nadam}
}

@article{pascanu_difficulty_2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1211.5063},
  primaryClass = {cs},
  title = {On the Difficulty of Training {{Recurrent Neural Networks}}},
  abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  language = {en},
  journal = {arXiv:1211.5063 [cs]},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  month = nov,
  year = {2012},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/3LFUJ87Q/Pascanu et al. - 2012 - On the difficulty of training Recurrent Neural Net.pdf},
  annote = {Comment: Improved description of the exploding gradient problem and description and analysis of the vanishing gradient problem}
}

@article{srivastava_dropout_nodate,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ``thinned'' networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  language = {en},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  pages = {30},
  file = {/Users/erichambro/Zotero/storage/CU76R2NH/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@article{r._j._williams_learning_1989,
  title = {A {{Learning Algorithm}} for {{Continually Running Fully Recurrent Neural Networks}}},
  volume = {1},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.2.270},
  number = {2},
  journal = {Neural Computation},
  author = {{R. J. Williams} and {D. Zipser}},
  month = jun,
  year = {1989},
  pages = {270-280}
}

@book{bird_natural_2009,
  address = {Beijing ; Cambridge [Mass.]},
  edition = {1st ed},
  title = {Natural Language Processing with {{Python}}},
  isbn = {978-0-596-51649-9},
  lccn = {QA76.73.P98 B57 2009},
  abstract = {This is an introduction to natural language processing, which supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation},
  language = {en},
  publisher = {{O'Reilly}},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  year = {2009},
  keywords = {Natural language processing (Computer science),Python (Computer program language),Python <Programmiersprache>,Sprachverarbeitung},
  file = {/Users/erichambro/Zotero/storage/8RGS76BC/Bird et al. - 2009 - Natural language processing with Python.pdf},
  note = {OCLC: ocn301885973}
}

@article{glorot_understanding_nodate,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  language = {en},
  author = {Glorot, Xavier and Bengio, Yoshua},
  pages = {8},
  file = {/Users/erichambro/Zotero/storage/V7U2KVAN/Glorot and Bengio - Understanding the difﬁculty of training deep feedf.pdf}
}

@inproceedings{papineni_bleu_2001,
  address = {Philadelphia, Pennsylvania},
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  shorttitle = {{{BLEU}}},
  doi = {10.3115/1073083.1073135},
  language = {en},
  booktitle = {Proceedings of the 40th {{Annual Meeting}} on {{Association}} for {{Computational Linguistics}}  - {{ACL}} '02},
  publisher = {{Association for Computational Linguistics}},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2001},
  pages = {311},
  file = {/Users/erichambro/Zotero/storage/29C9RWWB/Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine.pdf}
}

@article{Kiss:2006:UMS:1245119.1245122,
  title = {Unsupervised {{Multilingual Sentence Boundary Detection}}},
  volume = {32},
  issn = {0891-2017},
  doi = {10.1162/coli.2006.32.4.485},
  number = {4},
  journal = {Comput. Linguist.},
  author = {Kiss, Tibor and Strunk, Jan},
  month = dec,
  year = {2006},
  pages = {485-525},
  publisher = {{MIT Press}},
  location = {Cambridge, MA, USA},
  issue_date = {December 2006},
  numpages = {41},
  acmid = {1245122}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Mart\'\i{}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man\'e, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi\'egas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  note = {Software available from tensorflow.org}
}

@article{martins_softmax_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.02068},
  primaryClass = {cs, stat},
  title = {From {{Softmax}} to {{Sparsemax}}: {{A Sparse Model}} of {{Attention}} and {{Multi}}-{{Label Classification}}},
  shorttitle = {From {{Softmax}} to {{Sparsemax}}},
  abstract = {We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.},
  language = {en},
  journal = {arXiv:1602.02068 [cs, stat]},
  author = {Martins, Andr\'e F. T. and Astudillo, Ram\'on Fernandez},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/8BRDRPY3/Martins and Astudillo - 2016 - From Softmax to Sparsemax A Sparse Model of Atten.pdf},
  annote = {Comment: Minor corrections}
}

@article{pereyra_regularizing_2017,
  title = {{{REGULARIZING NEURAL NETWORKS BY PENALIZING CONFIDENT OUTPUT DISTRIBUTIONS}}},
  abstract = {We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.},
  language = {en},
  author = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, \L{}ukasz and Hinton, Geoffrey},
  year = {2017},
  pages = {11},
  file = {/Users/erichambro/Zotero/storage/PADHLTBW/Pereyra et al. - 2017 - REGULARIZING NEURAL NETWORKS BY PENALIZING CONFIDE.pdf}
}

@article{niculae_regularized_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.07704},
  primaryClass = {cs, stat},
  title = {A {{Regularized Framework}} for {{Sparse}} and {{Structured Neural Attention}}},
  abstract = {Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input. We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.},
  language = {en},
  journal = {arXiv:1705.07704 [cs, stat]},
  author = {Niculae, Vlad and Blondel, Mathieu},
  month = may,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/74Z9TEG3/Niculae and Blondel - 2017 - A Regularized Framework for Sparse and Structured .pdf},
  annote = {Comment: Published in NIPS 2017. 23 pages, including appendix}
}


