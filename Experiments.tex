\chapter{Experiments and Results}
\label{experiments_and_results}

\section{Experimental Setup} % (fold)
\label{sec:experimental_setup}

% section experimental_setup (end)

\section{Investigating Human Readable Channel} % (fold)
\label{sec:investigating_human_readable_channel}

% INVESTIGATION PURELY BASED ON NAME
\subsection{Comparing Baseline Models} % (fold)
\label{sub:comparing_baseline_models}

\subsubsection{Experiment Objective} % (fold)

As a baseline, we first wanted to investigate the informative power of simply the names of variables, with regards to different architectures of our baseline model.
Since good naming is a feature of good programming, we expected some reasonable signal to come from just the variable name, some signal should be learnable by both our seq-to-seq model and rote learner.

\subsubsection{Method} % (fold)

\begin{enumerate}
    \item For this experiment we used the deduplicated data set \mintinline{python}{UnsplitNoDuplicates}, as described in section \ref{sec:our_final_datasets}. 
    Given the lack of additional features such as code, we felt this removal of duplicates was a sensible way of removing an artificial improvement of our BLEU score. 
    \item We then generated a vocabulary for our training data by:
    \begin{enumerate}
        \item first generating a provisional vocabulary of tokens used more than 4 times in the training data.
        \item if these words existed in our Glove embedding vocabulary they were added to our the final vocabulary. We used the  \mintinline{python}{glove.6B.200d.txt} file of 200d embeddings trained on 6 billion tokens for our Glove embeddings, (CITE). 
        \item when this provisional list was exhausted we took the remaining most popular words, as defined by the Glove embedding file, to fill our final vocabulary to our designated "vocab-size"
    \end{enumerate}

    \item We then tokenized our data by: 
     \begin{enumerate}
         \item moving all argument descriptions to lower case, removing new lines, tokenizing them using the nltk punkt tokenizer (\mintinline{python}{nltk.word_tokenize}), replacing out of vocabulary tokens with an \mintinline{python}{"<UNK>"} token, and finally bookending our descriptions with \mintinline{python}{"<START>"} and \mintinline{python}{"<END>"} tokens
         \item we then tokenized our argument names by splitting them by character, and adding an \mintinline{python}{"<END_OF_ARG>"} token.
     \end{enumerate}

    \item Finally our models were trained in the standard way:
    \begin{enumerate}
        \item The Rote Learner was trained determininstically, in this case using `ngram overlap' feature, as mentioned in\ref{sec:rote_learner_model}. It was evaluated with different random seeds 50 times on the hold out dataset.
        \item The Seq-to-Seq modl was trained using the backpropagation algorithm and adaptive momentum optimisation. The hyperparameters are presented in table X. 80 Epochs were evaluated, and the best and average scores were taken \textbf{EDIT}
    \end{enumerate}

    \item Finally when it came to evaluating our models on held-out datasets, we would ran translations on the dataset and calculated the BLEU score using the BLEU method found in \textbf{HERE}.
    In particular BLEU scores were calculated over the whole corpus and \textbf{WHAT SMOOTHING HYPERPARAMTERS}.
    \item It should be stressed that if a hyperparameter had to be tuned, it was always done on the validation set, with the test set untouched, until final evaluation. In the case of all neural models, the results presented are using the best hyperparameters under the validation set.

\end{enumerate}

\subsubsection{Hyperparameters}

\subsubsection{Results} % (fold)

The results are displayed in Table \ref{table:name_baseline}. The hyperparameters for this models found in Table \ref{table:hyperparams_name_baseline}
\textbf{Visualise Attention}

\begin{table}[h!]
\begin{center}
\begin{tabular}{ c | c | c }
    Model                            & BLEU (Validation)  & BLEU (Test)    \\
    \hline
    Rote Learner (x50)                & $ 9.03242 \pm  0.31980 $ & $ 10.59658 \pm 0.26939 $   \\
    \hline
    Seq to Seq                        & $ 7.65221 $  & $ 7.63131  $ \\
    + \textit{attention}              & $ 9.77734 $  & $ 9.42463  $ \\
    + \textit{bidirectional encoder}  & $ 10.94970 $ & $ 10.83105 $ \\
    + \textit{dropout}                & $ 11.23303 $ & $ 11.30118 $ \\
    \hline
\end{tabular}
\caption {Results of Experiment \ref{sub:comparing_baseline_models}: Comparing Baseline Models }
\label{table:name_baseline}
\end{center}
\end{table}



% INVESTIGATION DIFFERENT TOKENIZATIONS
\subsection{Investigating Different Tokenizations} % (fold)
\label{sub:investigating_different_tokenizations}

\subsubsection{Experiment Objective} % (fold)

Having established a naive baseline, we decided to investigate the limits of how other named variables and function names could help our translation.
Once again we hypothesised that variable and function and other argument naming should play an important role, but that with this extra information, the attention in the LSTM would outpeform the rote learner in recognising the important parts of variable naming.

\subsubsection{Method} % (fold)

% subsubsection method (end)

\subsubsection{Hyperparameters}


\begin{table}[h!]
\begin{center}
\begin{tabular}{ c | c | c }
    Model                               & BLEU Validation            & BLEU Test  \\
    \hline
    \hline
    Rote Learner                        &                  & \\    
    - \textit{name only}                & $ 9.03242  \pm  0.31980 $ & $ 10.59658 \pm 0.26939 $  \\
    - \textit{name + function name}     & $ 12.22728 \pm  0.23408 $ & $ 14.33850 \pm 0.21622 $  \\
    - \textit{name + other args}        & $ 12.06262 \pm  0.16356 $ & $ 14.21676 \pm 0.16067 $  \\
    - \textit{name + function name + other args}  & $ 11.35728 \pm  0.13663 $ & $ 13.50626 \pm 0.13192 $ \\
    \hline
    \hline
    Seq to Seq                          &                  & \\
    - \textit{name only}                & $ 11.90863 $ & $ 11.79199 $ \\
    - \textit{name + function name}     & $ 10.79592 $ & $ 10.58280 $  \\
    - \textit{name + other args}        & $ 11.46794 $ & $ 11.23426 $ \\
    - \textit{name + function name + other args}   & $ 13.40158 $ & $ 12.95127 $ \\
    \hline
\end{tabular}
\caption {Investigate the effect of differnet code features}
\label{table:tokenization}
\end{center}
\end{table}





\subsection{Investigating Addition of Source Code} % (fold)
\label{subs:investigating_tokenizations_with_source_code}

Investigate the Best Tokenizations adding Source Code
\begin{table}[h!]
\begin{center}
\begin{tabular}{ c | c | c }
    Model                               & BLEU             & Perpelexity \\
    \hline
    \hline
    Rote Learner                        &                  & \\    
    - \textit{name only}                & $1.0 \pm 1.0 $   & \\
    - \textit{name + source code}       & $1.0 \pm 1.0 $   & \\
    \hline
    \hline
    Seq to Seq                          &                  &  \\
    - \textit{name only}                & $1.0 \pm 1.0 $   & \\
    \hline
    \hline
    Double Encoder to Seq               &                  &  \\
    - \textit{name + sourcecode}        & $1.0 \pm 1.0 $   & \\
    \hline
\end{tabular}
\caption {Investigate the descriptions with source code}
\label{table:human_source_code}
\end{center}
\end{table}

% subsection investigating_the_removal_of_duplicates (end)


% subsection results (end)

% section investigating_naturalness_hypothesis (end)

\section{Investigating the Computer Readable Channel} % (fold)
\label{sec:investigating_the_computer_channel}

\subsection{Comparing Code2Vec to Baselines} % (fold)
\label{sub:comparing_code2vec_to_baselines}

% subsection comparing_code2vec_to_baselines (end)

\begin{table}[h!]
\begin{center}
\begin{tabular}{ c | c | c | c }
    Model                             & BLEU (Unsplit)  & BLEU (Split)    & Perpelexity \\
    \hline
    Rote Learner (codeonly)           & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  & \\
    - softest                         & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  & \\
    - soft                            & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  & \\
    - hard                            & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  & \\
    - hardest                         & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  & \\
    \hline
    Code2Vec                          & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    \hline
\end{tabular}
\caption {Investigate the limits of purely codepaths}
\label{table:name_code2vec_solo}
\end{center}
\end{table}


\subsection{Comparing Code2Vec Altered} % (fold)
\label{sub:comparing_code2vec_altered}

% subsection comparing_code2vec_to_baselines (end)

\begin{table}[h!]
\begin{center}
\begin{tabular}{ c | c | c | c }
    Model                             & BLEU (Unsplit)  & BLEU (Split)    & Perpelexity \\
    \hline
    Rote Learner (best performer)           & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  & \\
    \hline
    Code2Vec                             & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    Code2Vec (masked)                    & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    Code2Vec (whole tree)                & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    \hline
\end{tabular}
\caption {Investigate code2vec but with masked variables}
\label{table:code_2_vec_masked}
\end{center}
\end{table}


\section{Investigating Combined Channels} % (fold)
\label{sec:investigating_combined_channels}

\subsection{Combined Code2Vec } % (fold)
\label{sub:combined_code2vec}

% subsection comparing_code2vec_to_baselines (end)

\begin{table}[h!]
\begin{center}
\begin{tabular}{ c | c | c | c }
    Model                             & BLEU (Unsplit)  & BLEU (Split)    & Perpelexity \\
    \hline
    Rote Learner  (code only)         & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    Code2Vec  (code only)             & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    \hline
    \hline
    Rote Learner  (name only)         & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    Seq2Seq  (name only)              & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    \hline
    \hline
    Rote Learner (combined)           & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  & \\
    Double Encoder                    & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    Code2Vec  + Char to Seq           & $1.0 \pm 1.0 $  & $1.0 \pm 1.0 $  &  \\
    \hline
\end{tabular}
\caption {Investigate code2vec combined with seq to seq}
\label{table:code2vec_embed}
\end{center}
\end{table}





\section{Analysis} % (fold)
\label{sec:analysis}

% section sundry_analysis (end)

\subsection{Investigating the Removal of Duplicates} % (fold)
\label{sub:investigating_the_removal_of_duplicates}

Investigate ND Datasets, see if performance improves.

\subsection{Visualising Attention Weights} % (fold)
\label{sub:visualising_attention_weights}

