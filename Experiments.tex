\chapter{Experiments and Results}
\label{experiments_and_results}


All experiments followed a similar general pattern of execution. 
This pattern of execution is described in detail in Section \ref{sec:experimental_setup}, which covers the general pipeline of the preparation, training and evaluation for each experiment.
Section \ref{sec:investigating_human_readable_channel} then describes the specific experiments and results of our investigations in to the human readable channel of code, whereas Section \ref{sec:investigating_the_computer_channel} covers the computer readable channel investivations.
The the results of the experiments combining these channels is presented in \ref{sec:investigating_combined_channels}, and further experiments into the dataset are presented in section \textbf{SECTION}


\section{Experimental Setup} % (fold)
\label{sec:experimental_setup}

\subsection{General Procedure} % (fold)
\label{sub:general_procedure}

% subsection general_procedure (end)

All our experiments were conducted in the same fashion, starting with a choice of dataset partition from Table \ref{table:thefinaldataset} that would be suitable to the investigation.
We would then tokenize the names and descriptions and other textual features, and if necessary extract the features we would from the code. 
These tokenization procedures and code extraction procedures are described in subsections \ref{sub:tokenizing_textual_input}, \ref{sub:tokenizing_argument_descriptions} and \ref{sub:tokenizing_code_features}.
After tokenization, the neural models would be trained on TESLA \textbf{GPUs}, for HOW long. 
These would evaluate performance on a dev set every epoch, and print translations.
How long did an epoch take.
When did we stop training.
Models were all evaluated by the same procedure as detailsed in section blah. 
%evaluated by assessing the BLEU, loss \& perplexity.


\subsection{Tokenizing Argument and Function Names} % (fold)
\label{sub:tokenizing_textual_input}

\subsection{Tokenizing Argument Descriptions} % (fold)
\label{sub:tokenizing_argument_descriptions}

\subsection{Tokenizing Code Features} % (fold)
\label{sub:tokenizing_code_features}

\subsection{Evaluation Procedure} % (fold)
\label{sub:evaluation_procedure}

% subsection evaluation_procedure (end)

% subsection tokenizing_code_features (end)

% subsubsection tokenizing_argument_descriptions (end)

% subsection tokenizing_textual_input (end)

% \subsubsection{Preparing the Data} % (fold)
% \label{ssub:Preparing the Data}

% \begin{enumerate}
%     \item All our data would come from one of the partitions of the dataset described in section \ref{sec:final_preparations}.

%     \item To tokenize variable names, function names and other arguments:
%     \begin{enumerate}
%         \item we generated a vocabulary of all possible tokens in valid python
%         \item We added separator tokens to the vocabulary, \mintinline{python}{"<SEPARATOR_1>"}
%         \item We then tokenizer as sequences of characters and adding an \mintinline{python}{"<END_OF_ARG>"} token to the end.
%     \end{enumerate}
    
%     \item To tokenize the argument descriptions:
%     \begin{enumerate}
%         \item We then generated a vocabulary for our training data by:
%         \begin{enumerate}
%             \item first generating a provisional vocabulary of tokens used more than 4 times in the training data.
%             \item if these words existed in our Glove embedding vocabulary they were added to our the final vocabulary. We used the  \mintinline{python}{glove.6B.200d.txt} file of 200d embeddings trained on 6 billion tokens for our Glove embeddings, (CITE). 
%             \item when this provisional list was exhausted we took the remaining most popular words, as defined by the Glove embedding file, to fill our final vocabulary to our designated "vocab-size"
%         \end{enumerate}
%         \item We then tokenized the descriptions by moving to lower case, removing new lines, and  tokenizing them using the nltk punkt tokenizer (\mintinline{python}{nltk.word_tokenize})
%         \item We then replaced out of vocabulary tokens with an \mintinline{python}{"<UNK>"} token, and finally bookending our descriptions with \mintinline{python}{"<START>"} and \mintinline{python}{"<END>"} tokens.
%         \item Path - seq?

%     \end{enumerate}



%     \item In cases of tokenized code: 
%      \begin{enumerate}
%          \item  In the case of tokenizing code paths, we first extracted the syntax tree from the source code, and traversed it exploring all paths between two nodes.
%          \item CONTINUE
%      \end{enumerate}

% \end{enumerate}

% Once these tokenizations were complete we were ready to train our models.

% \subsubsection{Training} % (fold)
% \label{ssub:training}


%     Our models were trained in the standard way:
%     \begin{enumerate}
%         \item The Rote Learner was trained determininstically, in this case using `ngram overlap' feature, as mentioned in\ref{sec:rote_learner_model}. It was evaluated with different random seeds 50 times on the hold out dataset.
%         \item The Seq-to-Seq model was trained using the backpropagation algorithm and adaptive momentum optimisation. The hyperparameters are presented in table X. 80 Epochs were evaluated, and the best and average scores were taken \textbf{EDIT}
%     \end{enumerate}




% \subsubsection{Evaluation} % (fold)
% \label{ssub:evaluation}

% % subsection training (end
% \begin{enumerate}



%     \item Finally when it came to evaluating our models on held-out datasets, we would ran translations on the dataset and calculated the BLEU score using the BLEU method found in \textbf{HERE}.
%     In particular BLEU scores were calculated over the whole corpus and \textbf{WHAT SMOOTHING HYPERPARAMTERS}.
%     \item It should be stressed that if a hyperparameter had to be tuned, it was always done on the validation set, with the test set untouched, until final evaluation. In the case of all neural models, the results presented are using the best hyperparameters under the validation set.

% \end{enumerate}

% section experimental_setup (end)

\section{Investigating Human Readable Channel} % (fold)
\label{sec:investigating_human_readable_channel}

% INVESTIGATION PURELY BASED ON NAME
\subsection{Comparing Baseline Models} % (fold)
\label{sub:comparing_baseline_models}

\subsubsection{Experiment Objective} % (fold)

As a baseline, we first wanted to investigate the informative power of simply the names of variables, with regards to different architectures of our baseline model.
We expected a reasonable signal to come from just the variable name which should be learnable by both our seq-to-seq model and rote learner. 

\subsubsection{Method \& Results} % (fold)

Since this experiment would only use the variable name to predict the descrption, we used the Filtered Random-Split Dataset listed in section.
This prevented inflation of BLEU scores due to exact matches being both in the train and validation set.

We then ran the standard tokenizations of input name on the Rote Learner, which only used the N-character-gram overlap criterion.
This was repeated 50 times, to provide an robust estimate of the standard deviation and mean BLEU scores, due to the inherent stocasticity to the model.

Then we ran a series of experiments on the basic Char-to-Seq architectures, starting from a basic seq-to-seq, then adding attention, a bidirectional encoder and finally dropout.
These best models were taken on the best BLEU scores up to 150 epochs, of training, and evaluated on the test and development set. % WHY 150

We report the BLEU scores in Table \ref{table:name_baseline} showing a strong performance from the Rote Learner of corpus level BLEU $9.03 \pm 0.32$ on validation and $10.60 \pm  0.30$. 
The Seq-to-Seq architectures initially underperformed the Rote Learner, scoring $7.76 $ and $  7.63$ on the validation and test set. However a marked improvement was seen by adding attention, as was adding the bidirectional encoder and dropout. 
The final model, with all addition beat the Rote Learner reasonably with a BLEU score of $12.46 $ and $12.72$ on validation and test set respectively.

The hyperparameters for these models are found in the Appendix, Table \ref{table:hyperparams_name_baseline}.



\begin{table}[!ht]
\begin{center}
\begin{tabular}{ c | c | c }
    Model                             & BLEU (Validation)  & BLEU (Test)    \\
    \hline
    Rote Learner (x50)                & $ 9.03242 \pm  0.31980 $ & $ 10.59658 \pm 0.26939 $   \\
    \hline
    Seq to Seq                        & $ 7.65221 $  & $ 7.63131  $ \\
    + \textit{attention}              & $ 9.77734 $  & $ 9.42463  $ \\
    + \textit{bidirectional encoder}  & $ 10.94970 $ & $ 10.83105 $ \\
    + \textit{dropout}                & $ 12.46441 $ & $ 12.72522 $  \\
    \hline
    % Seq to Seq                        & $ 4.60169 $ & $ 4.97880 $  \\
    % + \textit{attention}              & $ 4.81255 $ & $ 5.42022 $  \\
    % + \textit{bidirectional encoder}  & $ 3.00068 $ & $ 3.05952 $  \\
    % + \textit{dropout}                & $ 6.37864 $ & $ 6.33307 $  \\
    % \hline
\end{tabular}
\caption {Results of Experiment \ref{sub:comparing_baseline_models}: Comparing Baseline Models }
\label{table:name_baseline}
\end{center}
\end{table}



\subsubsection{Analysis} % (fold)

Most surprising of this result is the strength of the Rote Learner, despite there being no `exact matches' in the training and development sets. 
This is largely the result of there are arguments in the corpus with very similar descriptions, which may vary by only a few words. 

To investigate this we examined the predictions from the Rote Learner on the validation set. 
We evaluated the sentence level BLEU score on these, and found that although the vast majority evaluate to zero, a number of examples have a coincidental overlap in descriptions, resulting in a non negligible BLEU score. 
A random sample of these non-zero scored translations are presented in Figure \ref{tab:rotelearner_nameonly}.

With regard to the Seq-to-Seq, it is not surprising these standard additions to the seq-to-seq architecture improve the model. 
The addition of a bidirectional model adds capacity to the model by increasing the size of the hidden vector, and attention allows the model to condition sentence generation on the input, further adding complexity to the model.
The dropout also adds a regularizing effect.

It is perhaps surprising that the vanilla Seq-to-Seq struggles to outperform the Rote Learner. 
It is possible that the lack of contextualising information from attention results in an inability to learn from long description sequences that the Rote Learner can regurgitate immediately. 

However, once the attention and capacity are increased the results are explanatory.
The model synthesises similar (or even the same!) sequences of characters with their different descriptions, into a single description, which may fit better than randomly guessing from a bag of seen sequences. 

For instance, the most frequent argument, \mintinline[]{yaml}{name}, occurs in the train set 318 times with different descriptions. 
It also occurs 52 times in the validation set, again, with different descriptions. 
For each of these point the Rote Learner predicts a different description from the train set, on average scoring $0.00646 \pm 0.006$, whereas the Seq-to-Seq makes the same prediction each time, and scores $0.4029$, almost two orders of magnitude better, with the sensible average: ``name of the variable to return." 

In these cases the Seq-to-Seq model is interpolating in an overdetermined problem, and this highlights one of the advantages of neural networks in machine translation problems. 




\begin{table}
\begin{center}
\begin{tabular}{l}
\hline\\

\textbf{A Random Sample of Bleu Scores $>0$ from Rote Learner on Random Split }\\\\ 


\textbf{Bleu Score}: 0.364
\textbf{Confidence}: 100.0\%  \\
\textbf{Argument}: \mintinline[]{python}{e d g e _ m a t c h}\\
\textbf{Description}: a function that returns true if the edge attribute dictionary for \\
the pair of nodes ( u1 , v1 ) in g1 and ( u2 , v2 ) in g2 should be considered equal during\\
the isomorphism test . if edge\_match is not specified then edge attributes are not considered .\\
\textbf{Prediction}: a function that returns true if the edge attribute dictionaries for\\
the pair of nodes ( $<$UNK$>$ , $<$UNK$>$ ) in $<$UNK$>$ and ( u2 , $<$UNK$>$ ) in $<$UNK$>$\\
should be considered equal during matching .\\

\\
\textbf{B}: 0.075
\textbf{C}: 11.1\%  \\
\textbf{A}: \mintinline[]{python}{u s e _ l o c k i n g}\\
\textbf{D}: ` bool ` . if true use locks for update operation .\\
\textbf{P}: an optional ` bool ` . defaults to ` true ` . an optional bool . defaults to\\
true . if true , the assignment will be protected by a lock ; otherwise the behavior is\\
undefined , but may exhibit less contention .\\
\\
\textbf{B}: 0.558
\textbf{C}: 12.5\%  \\
\textbf{A}: \mintinline[]{python}{y _ t r u e}\\
\textbf{D}: ground truth ( correct ) target values .\\
\textbf{P}: ground truth ( correct ) labels .\\
\\
\hline
\\
\end{tabular}
\end{center}
\caption{This is a random sample of sentence level BLEU scores that are non-zero fon the validation set for a Rote Learner}
\label{tab:rotelearner_nameonly}
\end{table}


% INVESTIGATION DIFFERENT TOKENIZATIONS
\subsection{Investigating Different Tokenizations} % (fold)
\label{sub:investigating_different_tokenizations}

\subsubsection{Experiment Objective} % (fold)

Having established a naive baseline and the validity of our neural approaches, we decided to investigate how other human names could help our translation problem.
We hypothesised that the names in the function signature could play an important role in contextualising an overdetermined or underdetermined variable name, further improving the performance of the LSTM relative to the Rote Learner.

\subsubsection{Method \& Results} % (fold)

We decided to investigate how four different arrangements of input data would affect translation behaviour. 
These were:
just the argument name; the argument name with the function name; the argument name with other arguments; and the argument name with both function name and argument name.

Once again we used the Filtered Random-Split Dataset, to prevent advantages due to duplicates, and ran the standard tokenization procedures outline in Sections X. 

As in section \label{sub:comparing_baseline_models}, we ran 50 repetitions of the Rote Learner on these new tokenizations, to overcome the stochasiticity in their evaluations, once again using N-character-gram overlap as the matching criterion.

Then we ran four models of the Seq-to-Seq architectures, with attention, dropout and bidirectional encoders - just as the final model of section \label{sub:comparing_baseline_models}. These ran for a maximum of 150 epochs, and the model with best BLEU score was selected. 

We report the results in Table \ref{table:tokenization}, noting that the extra information most cases improves BLEU scoring.





The hyperparameters of these models are presented in appendix table. 


\subsubsection{Analysis} % (fold)
\label{ssub:analysis}


\begin{enumerate*}

 \item expect the rote learner to perform worse, sub strings causing problems. other args matching should be a problem! the longest argument gets matched! But the overlap in BLEU score is strong.
 \item why a lack of performance in te other areas?
  \item chekc the attention on the name only and others. can it tell separartors are helpful?
\end{enumerate*}



\begin{table}[!ht]
\begin{center}
\begin{tabular}{ c | c | c }
    Model                               & BLEU Validation            & BLEU Test  \\
    \hline
    \hline
    Rote Learner                        &                  & \\    
    - \textit{name only}                & $ 9.03242  \pm  0.31980 $ & $ 10.59658 \pm 0.26939 $  \\
    - \textit{name + function name}     & $ 12.22728 \pm  0.23408 $ & $ 14.33850 \pm 0.21622 $  \\
    - \textit{name + other args}        & $ 12.06262 \pm  0.16356 $ & $ 14.21676 \pm 0.16067 $  \\
    - \textit{name + function name + other args}  & $ 11.35728 \pm  0.13663 $ & $ 13.50626 \pm 0.13192 $ \\
    \hline
    \hline
    Seq to Seq                          &                  & \\
    - \textit{name only}    !!            & $ 12.46441 $ & $ 12.72522 $  \\
    - \textit{name + function name} !!    & $ 10.77699 $ & $ 11.24894 $ \\
    - \textit{name + other args}  !!    & $ 13.69964 $ & $ 13.73890 $  \\
    - \textit{name + function name + other args}   !! & $ 14.13267 $ & $ 13.63351 $ \\
    % \hdashline
    % Seq to Seq                          &                  & \\
    % - \textit{name only}                & $ 5.54103 $ & $ 5.43827 $  \\
    % - \textit{name + function name}     & $ 4.76165 $ & $ 4.40557 $  \\
    % - \textit{name + other args}        & $ 5.70546 $ & $ 6.04723 $  \\
    % - \textit{name + function name + other args}     & $ 6.26783 $ & $ 6.24153 $ \\
    
    \hline
\end{tabular}
\caption {Investigate the effect of different code features}
\label{table:tokenization}
\end{center}
\end{table}





% \subsection{Investigating Addition of Source Code} % (fold)
% \label{subs:investigating_tokenizations_with_source_code}

% Investigate the Best Tokenizations adding Source Code
% \begin{table}[!ht]
% \begin{center}
% \begin{tabular}{ c | c | c }
%     Model                               & BLEU             & Perpelexity \\
%     \hline
%     \hline
%     Rote Learner                        &                  & \\    
%     - \textit{name only}                & $1.0 \pm 1.0 $   & \\
%     - \textit{name + source code}       & $1.0 \pm 1.0 $   & \\
%     \hline
%     \hline
%     Seq to Seq                          &                  &  \\
%     - \textit{name only}                & $1.0 \pm 1.0 $   & \\
%     \hline
%     \hline
    % Double Encoder to Seq               &                  &  \\
%     - \textit{name + sourcecode}        & $1.0 \pm 1.0 $   & \\
%     \hline
% \end{tabular}
% \caption {Investigate the descriptions with source code}
% \label{table:human_source_code}
% \end{center}
% \end{table}

% subsection investigating_the_removal_of_duplicates (end)


% subsection results (end)

% section investigating_naturalness_hypothesis (end)

\section{Investigating the Computer Readable Channel} % (fold)
\label{sec:investigating_the_computer_channel}

\subsection{Comparing Code2Vec to Baselines} % (fold)
\label{sub:comparing_code2vec_to_baselines}


\subsubsection{Experiment Objective} % (fold)

In our first examination of the computer readable channel we were keen to investigate which features of the code might be most important.
That is, we wished to interrogate whether the full structure of differnet paths might be most important, or simply sub-sections of the path. 
For this reason we devised the four different matching schemes for the Rote Learner, enumerated in section Models.
We also simulataneously trained our modified Code2Vec model, to get an idea of whether neural models could outperform the rote learner.


\subsubsection{Method \& Results} % (fold)

By using the code2vec tokenization of paths outlined in section X, we extracted all paths for each argument in question from the AST of the code. 
The outputs of the result are presented in Table \ref{table:name_code2vec_solo}.



% subsection comparing_code2vec_to_baselines (end)

\begin{table}[ht!]
\begin{center}
\begin{tabular}{ c | c | c | c }
    Model                             & BLEU (Unsplit)  & BLEU (Split)    & Perpelexity \\
    \hline
    Rote Learner (codeonly)           &  & & \\
    - \textit{softest}                     & $ 0.70702 \pm  0.13979 $ & $ 0.73623 \pm 0.07094 $ &  \\
    - \textit{soft}                        & $ 6.93465 \pm  0.33987 $ & $ 7.09349 \pm 0.21836 $ & \\
    - \textit{hard}                        & $ 4.94109 \pm  0.28153 $ & $ 5.11432 \pm 0.18345 $ & \\
    - \textit{hardest}                     & $ 11.82453 \pm  0.15695 $ & $ 12.61319 \pm 0.12093 $ & \\
    \hline
    Code2Vec   !!                            & $ 18.93630 $ & $ 18.12909 $ & \\
    % \hdashline
    % Code2Vec                          & $ 12.64199 $ & $ 12.80257 $ & \\
    \hline
\end{tabular}
\caption {Investigate the limits of purely codepaths}
\label{table:name_code2vec_solo}
\end{center}
\end{table}

\subsubsection{Analysis} % (fold)
\label{ssub:analysis}

Most interestign to look at the attentions and vectors for code to vec. Is there any patter there?
What about the weight matrices: whats important, name or path?

% subsubsection analysis (end)

\subsection{Comparing Code2Vec Altered} % (fold)
\label{sub:comparing_code2vec_altered}

\subsubsection{Experiment Objective} % (fold)
Having established the original performance of the code2vec in principle, 
\begin{itemize}
    \item We masked the variable name - is it really code or lookign at variables?
    \item We mask the all variable - should be able to dediuce patterns.
\end{itemize}


\subsubsection{Method \& Results} % (fold)

By using the code2vec tokenization of paths outlined in section X, we extracted all paths for each argument in question from the AST of the code. 
The outputs of the result are presented in Table \ref{table:name_code2vec_solo}.

\subsubsection{Analysis} % (fold)

Checkout the weights now that the masking has taken place. Is it genuinely lerning good patters? What are good patterns? Whats an example of whats been classified and what being referred to in the syntax tree?


\begin{table}[ht!]
\begin{center}
\begin{tabular}{ c | c | c | c }
    Model                             & BLEU (Unsplit)  & BLEU (Split)    & Perpelexity \\
    \hline
    Rote Learner (best performer)          & $ 11.82453 \pm  0.15695 $ & $ 12.61319 \pm 0.12093 $ & \\
    \hline
    Code2Vec                !!              & $ 18.93630 $ & $ 18.12909 $ & \\
    Code2Vec (masked args)  !!              & $ 17.01868 $ & $ 16.93701 $ & \\                  
    Code2Vec (masked all)   !!               & $ 13.27677 $ & $ 13.07374 $ & \\
    % \hdashline

    % Code2Vec                              & $ 12.64199 $ & $ 12.80257 $ & \\
    % Code2Vec (masked args)                & $ 12.10462 $ & $ 11.93314 $ & \\
    % Code2Vec (masked all)                 & $ 9.01463 $ & $ 9.15697 $ & \\
    \hline
\end{tabular}
\caption {Investigate code2vec but with masked variables}
\label{table:code_2_vec_masked}
\end{center}
\end{table}


\section{Investigating Combined Channels} % (fold)
\label{sec:investigating_combined_channels}


\subsection{Combined Code2Vec } % (fold)
\label{sub:combined_code2vec}

% subsection comparing_code2vec_to_baselines (end)

\begin{table}[h!]
\begin{center}
\begin{tabular}{ c | c | c | c }
    Model                             & BLEU (Unsplit)  & BLEU (Split)    & Perpelexity \\
    \hline
    Rote Learner  (code only)        & $ 11.83875 \pm  0.15697 $ & $ 12.62479 \pm 0.12103 $ & \\
    Code2Vec  (code only)         !!    & $ 18.93630 $ & $ 18.12909 $ & \\
    % \hdashline
    % Code2Vec  (code only)             & $ 12.64199 $ & $ 12.80257 $ & \\
    \hline
    \hline
    Rote Learner  (name only)         & $ 19.03534 \pm  0.35183 $ & $ 19.68826 \pm 0.28100 $ & \\
    Seq2Seq  (name only)      !!         & $ 26.39189 $ & $ 25.39841 $ & \\
    % \hdashline
    % Seq2Seq  (name only)              & $ 15.94701 $ & $ 15.71091 $ & \\
    \hline
    \hline
    Rote Learner (combined)            & $ 15.01764 \pm  0.44897 $ & $ 15.75528 \pm 0.29560 $ & \\
    Code2Vec  + Char to Seq       !!     & $ 28.76581 $ & $ 27.68100 $ & \\
    % \hdashline
    % Code2Vec  + Char to Seq           & $ 23.11775 $ & $ 22.37520 $ & \\
    \hline
\end{tabular}
\caption {Investigate code2vec combined with seq to seq}
\label{table:code2vec_embed}
\end{center}
\end{table}

\subsubsection{Experiment Objective} % (fold)

\begin{itemize}
    \item finally we decide to combine
    \item we dont see a great improvemet separately, how about together?
    \item surprised
\end{itemize}


\subsubsection{Method \& Results} % (

\subsubsection{Analysis} % (fold)

Attentio here will be important. 
How do the code2vec vectors change direction when using names as well as code!
Weights etc

\section{Analysis} % (fold)
\label{sec:analysis}

% section sundry_analysis (end)

\subsection{Investigating the Removal of Duplicates} % (fold)
\label{sub:investigating_the_removal_of_duplicates}

\subsubsection{Experiment Objective} % (fold)

We now want to check whether generalisation is possible across results. 
We take the best models and repeat them on th esplit dataset.

\subsubsection{Method \& Results} % (


\begin{table}[!ht]
\begin{center}
\begin{tabular}{ c | c | c | c }
    Model                             & BLEU (Unsplit)  & BLEU (Split)    & Perpelexity \\
    \hline
    Rote Learner  (code only)         & $ 1.01257 \pm  0.11410 $ & $ 0.84707 \pm 0.04996 $ & \\
    Code2Vec                     !!   & $ 0.88741 $ & $ 0.89466 $ & \\
    Code2Vec  (mask args)             & $ 0.66894 $ & $ 0.67366 $ & \\
    Code2Vec  (mask alls)             & $ 0.69457 $ & $ 0.71360 $ & \\
    \hline
    \hline
    Rote Learner  (name only)         & $ 1.43480 \pm  0.16744 $ & $ 1.36063 \pm 0.06452 $ & \\
    \hline
    Seq2Seq                             & &  & \\
    - \textit{name only}               & $ 25.13928 $ & $ 24.16031 $ & \\
    - \textit{name + function name}     & $ 10.74375 $ & $ 10.43967 $ & \\
    - \textit{name + other args}        & $ 13.25115 $ & $ 13.64877 $ & \\
    - \textit{name + function name + other args}    & $ 12.35964 $ & $ 13.07744 $ &  \\
    \hline
    \hline
    Rote Learner (combined)            & $ 1.23201 \pm  0.16873 $ & $ 1.15672 \pm 0.10083 $ & \\
    Code2Vec  + Char to Seq        !!      & $ 1.84887 $ & $ 1.58697 $ & \\\\
    \hline
\end{tabular}
\caption {Investigate split datasets}
\label{table:split_datasets_embed}
\end{center}
\end{table}


\subsection{Visualising Attention Weights} % (fold)
\label{sub:visualising_attention_weights}

