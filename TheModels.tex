\chapter{Method}
\label{the_models}

% We focused on the signature because of the important nature of this section of the code. 
% In langugages such as C++, the signature of function may be all that is presented to the user of a third-party library, when using that library. 
% Therefore being able to generate reasonable descriptions from signature items alone would already be of value. 
% This does not seem an unrealistic target, since good developer practice often involves giving insightful names to variables and functions\footnote{https://github.com/google/styleguide/blob/gh-pages/pyguide.md\#s3.16-naming}.


\section{Overview}

In our investigations we focus on two main aspects of the code to generate descriptions for our arguments. 
These are the names in the function's signature, and the \textit{variable path-contexts} (VPCs) in the function's AST. 


In our experiments, we model the names of the signature as sequences of characters. We do this to capture the fact that variable and function names can often be composite and abbreviated - e.g. \mintinline[]{python}{web_ctx}. 
In this case both parts of the name might indicate a different clue to the argument:  the  \mintinline[]{python}{web} prefix may indicate a use in the internet domain; the \mintinline[]{python}{ctx} suffix may indicate a context object. 
In designing our models, we aim to pick up these patterns and conventions on the character-level.

We also focused on generating descriptions solely from the argument's VPCs. We feel that this most robust way of drawing inferences from the code, as it only examines instructions given directly to the computer. 
Any inferences here would be invariant under transformations such as renaming of variable. 
We hoped that by examining all the VPCs present for an argument, the model would learn representation for variables that indicate common usage (such as which methods are called on it), or perhaps even type.

% We chose to model this information as bag of modified `path-contexts' \cite{alon_general_2018} relevant to each variable. This would allow us to extract only the relevant sections of code pertintent to our chosen argument from the AST. 
% We hoped our models would then be able to learn which modified `path-contexts' are most informative, picking out the most relevant sections of code.

We prepared four models to investigate these data: A Rote Learner to act as baseline for our investigations; a character-level Seq2Seq Model to investigate the signature names; an original Code2Vec Decoder model to investigate the VPCs; and a Code2Vec + Seq2Seq Model to investigate both inputs combined.

The rest of this section is dedicated to presenting each of these models, summarised in Table \ref{tab:our_models_capability}, the  tokenization methods to obtain our data, and the implementation in conducting our experiments.



\begin{table}[tb]
    \centering

    \begin{tabular}{c  c  c}
          Model & Uses Signature Data & Uses VPC Data \\ 
    \hline
    Rote Learner & \checkmark & \checkmark \\
    Seq2Seq Decoder & \checkmark & \\
    Code2Vec Decoder &    &  \checkmark \\
    Code2Vec + Seq2Seq Decoder& \checkmark & \checkmark \\
    \hline
    \end{tabular}
    \caption{Overview of Our Models}
    \label{tab:our_models_capability}
\end{table}











\section{The Models}

\subsection{Rote Learner Model} % (fold)
\label{sec:rote_learner_model}

Our Rote Learner model was designed to act as a strong benchmark in all our investigations. 
It was designed according to a simple principle: \textit{the Rote Learner generates a description from a test point by returning in full a random description from a list of best-matching training point.}
It is defined formally in Algorithm \ref{alg:rote_learner_general}. 

The benefit of this model is that the definition of `best-matching' can then be changed according to the modality of the input data. In the sections below we present a brief description of these matching algorithms, with an appendix displaying each algorithm in pseudocode. A summary of these matching algorithms is presented in Table \ref{tab:matching_summary}, 

\begin{algorithm}
    \caption{The general Rote Learner algorithm }
    \label{alg:rote_learner_general}
    \begin{algorithmic}
        \Procedure{GenerateDescription}{$t$}\Comment{Generate a description for test argument $t$ }
        \State $\mathcal{M} \gets \text{BestMatchingSet}(t, training\_points)$
        \State $x \gets \text{RandomChooseOne}(\mathcal{M})$
        \State $d \gets \text{GetDescription}(x)$
        \State \textbf{return} $d$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{enumerate}
    \item \textbf{NCharacter-Gram Overlap} \textit{(Sig. Data)} \\This matching criterion was used for function signature data, and so operates on sequences of characters. It compiles a list of the training points which have the longest n character overlap with the test data point (or the longest common substring). If the same point has two of such overlaps, it is included twice, and so forth. This pseudo code is presented in Appendix Algorithm \ref{alg:ngram_overlap}.
    \item \textbf{Proportional Contexts} \textit{(VPC Data)}\\This criterion matches VPCs by treating each VPC as an atomic unit. For each VPC within the test point, the criterion finds all the training points with the same VPC. It combines these into one list, returns this list. Naturally, the list may have duplicates of the same training point. This is equivalent to choosing the training points proportionally to how often they have an overlapping VPC with the test point. The pseudocode is presented in Appendix Algorithm \ref{alg:context_algorithm} 
    \item \textbf{Max Contexts} \textit{(VPC Data)}\\This criterion takes the list output by the \textbf{Proportional Context} criterion, and only chooses the training points that appear most frequently in the list. This is equivalent to choosing the training points that have the greatest number of overlapping path-contexts with the test point. The pseudocode is also presented in Appendix Algorithm \ref{alg:context_algorithm} 
    \item \textbf{Proportional SubContexts} \textit{(VPC Data)}\\This criterion also match VPCs of the data, but treats each VPC as sequence of nodes. Specifically, as it creates $q = p{\downarrow}x_f$ and treats the new path, $q$, as the sequence of nodes and arrows that compose it. This allows it to match subpaths. The matching criterion takes the test point, and for each VPC within it, finds the training VPC with the longest subsequences that matches it. It collects the corresponding datapoints to these paths and combines them into one list. It then returns this list. This is equivalent to choosing the training points that best match each VPC in the test point, proportionally to how often they are the best match. The pseudocode is presented in Appendix Algorithm \ref{alg:sub_context_algorithm} 
    \item\textbf{Max SubContexts} \textit{(VPC Data)}\\The matching criterion takes the list output by the \textbf{Proportional SubContext}, and only chooses the training points that appear most frequently in the list. This is equivalent to choosing the training points that most often are the best match for all path-contexts of the test. The pseudocode is also presented in Appendix Algorithm \ref{alg:sub_context_algorithm} 
    \item\textbf{[Combinations]}\textit{(Sig. Data + VPC Data)} \\ Naturally these criteria can be combined across modalities to act upon combinations of the function signature and AST. In cases where this is done, the new list is simply the combination of the lists from the combined individual criteria. 
\end{enumerate}









\begin{table}[h!]
\makebox[\linewidth][c]{
    \centering

    \begin{tabular}{l c c p{9cm}}
    \hline
    BestMatchingSet & Use Sig. & Use AST & Description: \textit{choose list of training points...} \\
    \hline
    \hline
    NCharacter-Gram Overlap     & \checkmark & & 
         which have the longest n-character gram overlap with test \\
    % \hline
    % \hline
  
    Proportional Context  & & \checkmark & 
        which match each VPC in test, and combine\\
    Max Context  & & \checkmark &
        which match each VPC in test, combine, and take most frequent\\

    Proportional SubContext  & & \checkmark &
        which \textit{best-effort} match each VPC in test, and combine\\
    Max SubContext  & & \checkmark & 
        which \textit{best-effort} each VPC in test, combine, and take most frequent\\
    Combinations  &\checkmark  & \checkmark & 
        from the combined the lists from each BestMatchingSet\\
    \hline

    \hline
    \end{tabular}
}

    \caption{A summary of the BestMatchingSet algorithms we use in the experiments. For sake of simplicity, a \textit{best-effort} match here refers to the match with longest matching subsequence, when treating a VPC as one long sequence}
    \label{tab:matching_summary}
\end{table}





% These chose a description from a set of training samples by:
%     \begin{itemize}
%         \item combining samples with largest subpath overlap for each path in test, and chosing from that list. (Sub Path Proportional)
%         \item combining samples with largest subpath overlap for each path in test, and chosing from the most frequent training point(s) in that list. (Sub Path Max)
%         \item combining samples full overlap for each path in test, and chosing from that list (Full Path Proportional)
%         \item combining samples full overlap for each path in test, and chosing from the most frequent training point(s) in that list. (Full Path Max)
%     \end{itemize}








% subsection rote_learner_model (end)

\subsection{Seq2Seq Model} % (fold)
\label{sec:character_level_sequence_to_sequence}

\subsubsection{Overview}

Our character level sequence-to-sequence model follows the standard formulation as found in Sutskever et al \cite{sutskever_sequence_2014} \cite{bahdanau_neural_2014}, but encodes a sequences of characters, and decodes a sequence words. 
We use a bidirectional LSTM as our encoder, concatenating its outputs\cite{bahdanau_neural_2014}, and use a single LSTM as a decoder.
We also use a Luong attentional mechanism \cite{luong_effective_2015} over encoder outputs, throughout our decoding.

Since our vocabulary of characters is size 70, we embed our input into 70-dimensional space, with trainable embeddings. We initialising these with a one-hot encoding.
Our characters are embedded into a 200 dimensional space use Glove embeddings for our decoder. 
These are not made trainable, due to concerns that the model may overfit. 
We apply dropout \cite{srivastava_dropout_nodate} to inputs, outputs and states in both the encoder and decoder, and clip gradients to magnitude 1. The dropout fraction is a tuned hyperparameter.
We define our object function as the minibatch average of the summed cross entropy losses between the true words and the distributions over words at each time step.
We train our model by minimising this with Adam \cite{kingma2014adam}. 

We present the model in full formality below. In it we present a non-bidirectional encoder, and treat the modification for bidirectionality separately, since we experiment with both variations of model.

\subsubsection{Encoder}

First we iteratively embed each sequence of input tokens $\{x_1,...x_k\}$ into its vector representation $\{\textbf{x}_1,...\textbf{x}_k\}$, according to:
\begin{align}
    \textbf{x}^T_t = \mathbf{H}^C_{x_t,:}
\end{align}
where 
$\textbf{H}^C \in \mathbb{R}^{V_c\text{x}D_c}$ is our character embedding matrix, 
$V_c$ is our character vocabulary, 
$D_c$ is our encoding dimension, 
and $\textbf{X}_{i,:}$ indicates selection of row $i$ from a matrix \textbf{X}.

Then we define an LSTM with functions $f$ and $g$, which takes input vector $\textbf{x}_t$ and generates an output, $\textbf{h}_t$, and updates its own state $\textbf{c}_t$ according to the equations in Equations \ref{eq:lstm1} to \ref{eq:lstm6}.
\begin{align}
    \textbf{h}_t = f(\textbf{x}_t, \textbf{h}_{t-1}, \textbf{c}_{t-1}) \label{eq:s2sh}\\
    \textbf{c}_t = g(\textbf{x}_t, \textbf{h}_{t-1}, \textbf{c}_{t-1}) \label{eq:s2sc}
\end{align}

With these sequences of vectors we then encode each vector $\{\textbf{x}_1,...,\textbf{x}_k\}$ with the LSTM until we have generated outputs $\{\textbf{h}_1,...,\textbf{h}_k\}$ and a intermediate fixed length representation, $s$ of the sequence: 
\begin{align}
     s = (\textbf{h}_k, \textbf{c}_k) \label{eq:s2s_state}
\end{align}

\subsubsection{Decoder}

We then seek to iteratively decode the our sequence of word vectors $\{\textbf{y}_1,..., \textbf{y}_l\}$ given this state $s$ such that:
\begin{align}
    P(\textbf{y}_1,...,\textbf{y}_l) = \prod_{i=1}^l p(\textbf{y}_i| \{\textbf{y}_1,...,\textbf{y}_{i-1}\}, s)  = \prod_{i=1}^ld(\mathbf{y}_{i-1}, \mathbf{h}'_{i-1},\mathbf{c}'_{i-1} )
\end{align}
where $\mathbf{h}'_{i},\mathbf{c}'_{i} $ are the hidden states of the decoder LSTM at step $i$, $\mathbf{y}_0$ is start-of-sentence vector, and $d$ is a general decoding method using the LSTM which we illustrate below.

First we initialise our decoders hidden state ($\textbf{h}'_0$,$\textbf{c}'_0$) with the our intermediate state $s$:
\begin{align}
    (\textbf{h}'_0, \textbf{c}'_0) \gets s = (\textbf{h}_k, \textbf{c}_k) \label{eq:s2s_assign}
\end{align}
Then we iteratively generate $\textbf{y}_t = d(\mathbf{y}_{t-1}, \mathbf{h}'_{t-1},\mathbf{c}'_{t-1} ) $. We do this by performing:

\begin{align}
    \textbf{h}'_t = f(\textbf{y}_t, \textbf{h}'_{t-1}, \textbf{c}'_{t-1}) \label{eq:s2syh}\\
    \textbf{c}'_t = g(\textbf{y}_t, \textbf{h}'_{t-1}, \textbf{c}'_{t-1}) \label{eq:s2syc} \\
    \textbf{w}_{t+1} = softmax(\textbf{W}_p\textbf{h}'_t) \label{eq:decoder_w}  \\
    w_{t+1} = argmax(\textbf{w}_{t+1})  \\
    \textbf{y}^T_{t+1} = \textbf{H}^W_{w_{t+1}:}  
\end{align}
where $\textbf{H}^W \in \mathbb{R}^{V_w\text{x}D_w}$ is our word embedding matrix, 
$\textbf{W}_p \in \mathbb{R}^{V_w\text{x}L}$ is a projection matrix, $V_w$ is our word vocabulary size, 
$D_w$ is our word embedding dimension, 
$\mathbf{h_i} \in \mathbb{R}^L$,
$L$ is the dimension of our decoding LSTM,
and $\textbf{X}_{i,:}$ indicates selection of row $i$ from a matrix \textbf{X}.

The above routine is performed at inference time. However, during training we choose   $w_{t+1} $ to be the next word token  in the true sequence $\{y_1,...y_l\}$, instead of the most likely next word generated by the decoder. Therefore in this case $w_{t+1}  = y_{t+1}$ \textbf{not}  $w_{t+1} = argmax(\textbf{w}_{t+1})$. This is known as \textit{teacher forcing} and is known to produce better results for these models\cite{Goodfellow:2016:DL:3086952}\cite{r._j._williams_learning_1989}.

\subsubsection{Bidirectional LSTM}

The above framework is modified when dealing with a bidirectional LSTM encoder. The bidirectional LSTM consists of two LSTMs, one which processes the vectors forward $\{\textbf{x}_1,...\textbf{x}_k\}$, producing  $\{\overrightarrow{\textbf{h}}_1,...\overrightarrow{\textbf{h}}_k\}$, and one which processes the vectors in reverse order  $\{\textbf{x}_k,...\textbf{x}_1\}$, producing  $\{\overleftarrow{\textbf{h}}_1,...\overleftarrow{\textbf{h}}_k\}$.

Following Bahdanau's example \cite{bahdanau_neural_2014} we concatenate these vectors to make a new vector of twice the length. Otherwise, the model is identical:
\begin{align}
(\textbf{h}^b_i)^T = [\overrightarrow{\textbf{h}}^T_i,\overleftarrow{\textbf{h}}^T_i ] \\
(\textbf{c}^b_i)^T = [\overrightarrow{\textbf{c}}^T_i,\overleftarrow{\textbf{c}}^T_i ] \\
s = (\textbf{h}^b_k, \textbf{c}^b_k)
\end{align}

\subsubsection{Attention}

To make the adjustment of adding attention, we define a modification to the above decoding sequence. Since we use Luong-style\cite{luong_effective_2015} global attention, we incorporate a contextualising vector, $\textbf{z}_t$, to the output of our LSTM at each step, before feeding through our final softmax. This is equivalent to replacing equation \ref{eq:decoder_w} in the Decoder section with the following two equations:
\begin{align}
    \tilde{\textbf{h}}'^a_t =  \text{tanh}(\textbf{W}_a[\textbf{z}_t, \textbf{h}'_t])\\
    \textbf{w}_{t+1} = softmax(\textbf{W}_p\tilde{\textbf{h}}'^a_t) \label{eq:s2syw}  
\end{align}
where $\textbf{W}_a \in \mathbb{R}^{(L)\text{x}(A+L)} $ is our attention matrix, and 
$A$ is our attention vector size.

This attention vector $\textbf{z}_t$ is calculated as the weighted average over  $\{\textbf{h}_1,...,\textbf{h}_k\}$ corresponding to their scores in vector $\textbf{a}_t = [a_t^1,...,a_t^k]$ where:

\begin{align}
    a_t^i = \dfrac{exp(\mathbf{h}'^{T}_t\mathbf{h}_i)}{\sum_{j=1}^kexp(\mathbf{h}'^{T}_t\mathbf{h}_i)} 
\end{align}

\subsubsection{Dropout}

We added random dropout layer to the three encoder and three decoder variables during training:
$\{\textbf{x}_j, \textbf{y}_j, \textbf{h}_j, \textbf{h}'_j, \textbf{c}_j, \textbf{c}'_j\}$.  This meant that $f \in [0,1]$, fraction of elements that are set to 0 at random from a vector or matrix, during training, which has been shown to have a regularising effect  \cite{srivastava_dropout_nodate}. In our case $f$ was a tuned hyperparameter. 

\subsubsection{Optimisation}

We define the words in our true sequence as $\{y_1, ..., y_l\}$, and their one hot representations as $\{\mathbb{I}_1, ...,\mathbb{I}_l \}$.
In decoding we calculate distributions for each target word in the sequence $\{\mathbf{w}_1, ..., \mathbf{w}_l\}$.
We define our objective function for one point as the sum of cross entropy losses between the target words in our sequence, (represented as one hot vectors) and the corresponding distributions of those words.
\begin{align}
    \mathcal{L}_{single} = -\sum_{i=1}^l\mathbb{I}_i\cdotp\log(\mathbf{w}_i)
\end{align} 
For multiple points, we take the average over the minibatch, so the losses dont change dramatically with changing batch size.
\begin{align}
    \mathcal{L}_{mb} = -\frac{1}{N}\sum_{j=1}^N\sum_{i=1}^l\mathbb{I}_i\cdotp\log(\mathbf{w}_i^j)
\end{align}.

We use Adam \cite{kingma2014adam} to minimise this objective function. Adam is a stochastic gradient descent mechanism, that uses a momentum based update that adapts during training. It also maintains a separate learning rate for each parameter, and makes adjustments of this based on the gradients and square gradients. This optimiser has proven to be highly effective in training neural networks\cite{ruder_overview_2016}.

Finally we also apply gradient clipping, where gradients are constrained have a certain magnitude:
\begin{align}
     g' = min(-x,max(g, x))
\end{align}
where $g$ is our old gradient, $x \in \mathbb{R}^{+}$, and $g'$ our clipped gradient. This gradient clipping prevents large shifts occuring in minima next to steep gradients, has been shown to facilitate training\cite{pascanu_difficulty_2012}.

\subsection{Code2Vec Decoder Model} % (fold)
\label{sec:code2vec_decoder_model}

\subsubsection{Overview}

Our Code2Vec Decoder model creates a modified version of Alon et al's \cite{alon_code2vec_2018} Code2Vec code-vector, and uses that as intermediate representation for decoding. We differ from Alon et al's paper by building our vector from the our argument's \textit{variable path-contexts}, instead whole AST's \textit{path-contexts}.
We use the same decoder model as the Seq2Seq (without attention).

We encode our VPCs using a 300-dimensional vector for the path, and 300-dimensional vector for the terminal node, and produce a final code vector of size 300.
This is used as both components of our intermediate state, to be decoded our LSTM decoder, which is the same as the Seq2Seq model.
We apply a dropout around the intermediate context vectors, and also the inputs, outputs and hidden states of the decoder. 
We use the same objective function, gradient clipping and optimizer as the Seq2Seq model.

\subsubsection{Encoder}

In this model our input for each argument is a set of \textit{variable path-contexts}, $\{v_1,...v_k\}$, where each $v_i = (p_i, n_i)$ and  $p_i$ is a path, and $n_i$ is a terminal node. These paths and nodes are encoded simply as integers, indicating items in a vocabulary.

For each $v_i$ we then prepare an embedded context vector $\mathbf{c_i}$ as follows:
\begin{align}
\textbf{p}_i = \mathbf{H}^P_{p_i,:}\\
\textbf{n}_i = \mathbf{H}^N_{n_i,:}\\
\tilde{\textbf{c}_i} = ([\textbf{p}_i , \textbf{n}_i])^T \\
\textbf{c}_i = \text{tanh}(\mathbf{W_c}\tilde{\textbf{c}_i}  + \mathbf{b_c})
\end{align}
Where  $\textbf{H}^P \in \mathbb{R}^{V_p\text{x}D_p}$ is our path embedding matrix, 
$\textbf{H}^N \in \mathbb{R}^{V_n\text{x}D_n}$ is our terminal node embedding matrix, 
$\textbf{W}_c \in \mathbb{R}^{D_c\text{x}(D_p+D_n)}$,
$\textbf{b}_c \in \mathbb{R}^{D_c}$,
$V_p$ is the size of our path vocabulary,
$V_n$ is the size of our terminal node vocabulary,
$D_c$ is the size of our context vector,
$D_p$ is the size of our path embedding vector,
$D_n$ is the size of our terminal node embedding vector, and
and $\textbf{X}_{i,:}$ indicates selection of row $i$ from a matrix \textbf{X}.

The final code2vec-style vector $\textbf{v}$ is calculated as the weighted sum over  $\{\textbf{c}_1,...,\textbf{c}_k\}$ corresponding to their scores in vector $\textbf{a} = [a^1,...,a^k]$ where:
\begin{align}
    a^i = \dfrac{exp(\mathbf{\alpha}^{T}\mathbf{c}_i)}{\sum_{j=1}^kexp(\mathbf{\alpha}^{T}\mathbf{c}_i)}
\end{align}
and $\mathbf{\alpha} \in \mathbb{R}^D_c$ is a global attention parameter that is learnt.
We use this vector $\mathbf{v}$ to present our final intermediate encoding as :
\begin{align}
     s = (\mathbf{v}, \mathbf{v}) 
\end{align}

\subsubsection{Decoder, Optimisation}

We use the same decoder as presented in the Seq2Seq model.
We also use the same optimiser, objective function, and gradient clipping was presented in the Seq2Seq model.

\subsubsection{Dropout}

Throughout the network, we added random dropout layers to concatenated context vector and the three decoder variables during training:
$\{\tilde{\textbf{c}_i}, \textbf{y}_j, \textbf{h}'_j,  \textbf{c}'_j,\}$
where our dropout fraction was tuned as a hyperparameter. 

% subsection code2vec_to_sequence_model (end)

\subsection{Code2Vec + Seq2Seq Decoder} % (fold)
\label{sub:code2vec_sequence_to_sequence}

\subsubsection{Overview}

In this model we used both sources of information for our decoder, by concatenating two separate encoded vectors, and passing this through a linear layer. 
This enabled the model to choose which sections of information were most important.
We use the identical set up as the Code2Vec Decoder to generate our code vector, and the identical set up to Seq2Seq for the character encoder and word decoder, and we used the same dropouts, objective function, gradient clipping and optimiser as both.

\subsubsection{Encoder}

In this model we consider the two encoders above as operators that return tuples. We first the concatenate each component of tuple:
\begin{align}
    s^{s2s}  \gets \mathcal{SEQ}(\mathbf{x}) \\
    s^{c2v} \gets \mathcal{COD}(\mathbf{x}) \\
    (\mathbf{s}_1^{comb})^T = [ (\mathbf{s}^{s2s}_1)^T , (\mathbf{s}^{c2v}_1)^T ] \\
    (\mathbf{s}_2^{comb})^T  =  [ (\mathbf{s}^{s2s}_2)^T , (\mathbf{s}^{c2v}_2)^T ]  
\end{align}
where $\mathcal{SEQ}$ returns the encoder component of the Seq2Seq model,
$\mathcal{COD}$ returns the component of the Code2Vec Decoder, 
$s^\mathcal{X}$ indicates the a tuple of vectors from encoder operation $\mathcal{X}$.

Then each component is fed through a linear layer before combining into the final encoded state to be decoded:
\begin{align}
    \mathbf{s}_1 = \mathbf{W}_d\mathbf{s}^{comb}_1 + \mathbf{b}_d\\
    \mathbf{s}_2 = \mathbf{W}_d\mathbf{s}^{comb}_2 + \mathbf{b}_d\\
    % \mathbf{W}_d + \mathbf{b}_d
    s = (\mathbf{s_1}, \mathbf{s_2})
\end{align}
where $\mathbf{W}_d \in \mathbb{R}^{L\text{x}L_{comb}}$, 
$\mathbf{b}_d \in \mathbb{R}^{L}$,
$L$ is the decoder size,
$L_{comb}$ is the size of the combined concatenated vector.


\subsubsection{Decoder, Attention, Dropout, Optimisation}
We use the same decoder as presented in the Seq2Seq model, and use Luong Attention in the identical manner.
We also use the same optimiser, objective function, and gradient clipping was presented in the Seq2Seq model.
We used dropout any variables specified in either the Seq2Seq Model dropout section or the Code2Vec Decoder model dropout section.


\section{Tokenizations} % (fold)
\label{sec:tokenizations}

\subsection{Tokenizing Signature Data}

Since signature data was being investigated character-wise, our encoding vocabulary was short and complete - constrained to 70 characters in all.

If our investigations required just one name, we encoded it as is, and appended a end-of-sequence ($<$EOS$>$) token to the list of characters.
If we required multiple names, we concatenated the strings, using separator tokens ($<$S$>$) in between, which would indicate the type of the previous name. Separators tokens could then be replaced by single characters invalid in a python name, for representation as a string.

\begin{figure}[h!]

\hrulefill\\
    \textbf{signature:} \mintinline[]{python}{def funcA(foo, barA, barB):}\\
    \textbf{argument:} \mintinline[]{python}{foo} \\
    \textbf{tokenization type:} variable name $+$ function name $+$ all coarguments \\
    \textbf{tokenized (sequence):} \mintinline[]{python}{f,o,o,<S1>,f,u,n,c,A,<S2>,b,a,r,A,<S3>,b,a,r,B,<S3>,<EOS>}\\
    \textbf{tokenized (string):} \mintinline[]{python}{"foo|funcA-barA+barB+?"}\\
\hrulefill\\ 
    \caption{An example of a tokenization of the signature data}
    \label{fig:tokenizatio}
\end{figure}

\subsection{Extracting \& Tokenizing VPC Data }
\label{sub:code_tokenization}
Although in principle, the VPCs could be taken directly from the entire AST of the accompanying source, a few small modifications were made to facilitate training, and reduce the redundant VPCs in the model. 
First the AST was pruned to remove terminal nodes that not variables, such as \mintinline[]{yaml}{Load} or \mintinline[]{yaml}{Store} nodes. Such nodes are visible in Figure \ref{fig:fullAST}. 

Although these nodes carry useful information as to whether the target name was being loaded or stored, they almost doubled the number of terminal nodes, and therefore quadratically increase the number of \textit{path-contexts} in the AST. Given our memory and time considerations in both training and preparation we pruned these nodes, noting that the path $p$ for any VPCs eliminated would still be present in the corresponding variable node to which \mintinline[]{yaml}{Load} or \mintinline[]{yaml}{Store} refers. 
Although the loading and storing information had been removed, the tree still contained a rich information about the relationships between variable - the investigation of Section \ref{the_dataset}, for instance, was done after pruning.

Finally, in extracting VPCs we also removed the paths that led to the \mintinline[]{yaml}{arguments} series of nodes, and the string node child of \mintinline[]{yaml}{FunctionDef} . These nodes contained information such as the names of the arguments or name of the function. Since we wanted the function signature data to be investigated separately we removed these paths. 

Finally we had our VPCs and could prepare to tokenize them. To do this we took the $V_p$ most common paths and the $V_n$ most common terminal nodes in the training set, and used these our respective vocabularies.  All other paths and nodes were given an $<$UNK$>$ token in encoding. $V_p$ and $V_n$ were hyperparameters we tuned given the resources of the models.

\subsection{Tokenizing Descriptions}
\label{sub:tokenize_descriptions}

Finally we tokenized all descriptions by setting them to lower case and using the \mintinline[]{python}{nltk} \cite{bird_natural_2009} Punkt \cite{Kiss:2006:UMS:1245119.1245122}tokenizer. We then formed a vocabulary of size $V_w$ from the training set and GloVe embeddings. We did this by taking all words in the training set that had appeared at least 5 times and if they appeared in the GloVe vocabulary, adding them to our training vocabulary. Once this was done, we filled the remaining space in the vocabulary with the most popular words in GloVe that had not already been included. This was done to minimize out-of-vocabulary ($<$UNK$>$) tokens. This tokenized training vocabulary was then used for all learners.

\section{Evaluation}

To evaluate our quantively models we used the corpus BLEU-4 metric\cite{papineni_bleu_2001}, common in machine translation. This is a metric that has been shown to correlate with fluency and adequancy of translations. 

To calculate BLEU, we first define a modified ngram precision, $p_n$ as:
\begin{align}
    p_n = \dfrac{\sum_{ngram \in \mathcal{C}} Count_{clip}(ngram)}{\sum_{ngram' \in \mathcal{C}} Count(ngram')} \nonumber b
\end{align}
where $Count(ngram)$ counts the number of ngrams in the candidate sentence, and  $Count_{clip}(ngram)$ counts the number of ngrams in the candidate sentence, up to the maximum number of those ngrams that exist in any reference sentence:
\begin{align}
Count_{clip}(ngrams) = min(Count(ngrams),MaxRefCount(ngrams)) \nonumber
\end{align}

We then define a brevity penalty, to penalise high precision scores, when candidates of length $c$ are much shorter than references of length $r$.
\begin{align}
BP = \begin{cases}
      1 & \text{if}\ c > r \\
      e^{1-r/c} & \text{if}\ c \leq r
    \end{cases}\nonumber
\end{align}

The sentence-level BLEU-4, is then brevity penalty multiplied by the geometric mean of the modified ngram precisions, up to 4-gram:
\begin{align}
\text{BLEU-4} = BP\cdotp\prod_{n=1}^4 p_n^{\frac{1}{4}}
\end{align}

This metric can be very harsh on individual sentences, so on a corpus level, we take our modified precisions by summing ngram matches sentence by sentence first.
\begin{align}
    p_n = \dfrac{
    \sum_{C \in \{Candidates\}}\sum_{ngram \in \mathcal{C}} Count_{clip}(ngram)}{
    \sum_{C' \in \{Candidates\}}\sum_{ngram' \in \mathcal{C'}} Count(ngram')} \nonumber 
\end{align}
We then calculate our length $c$ and $r$ from the brevity penalty as the total length of the candidates and references respectively.
With these modification, the calculation of BLEU-4 is identical

In our experiments we used corpus BLEU-4 for evaluating our models, though occasionally examined sentence BLEU-4 for our analysis. 
For a qualitative evaluation, we always interrogated the generated descriptions, source code and gold descriptions directly.


\section{General Procedure}
\label{sec:general_procedure}

Our general experimental procedure was straightforward. We would take our chosen dataset from Table \ref{tab:final_datasets} and  tokenize it as above. Then we would feed these into our own implementations of models, written in Tensorflow\cite{tensorflow2015-whitepaper}. Variables in these models were all initialised with Xavier initialisation \cite{glorot_understanding_nodate}. Since our objective function does not correspond well with our evaluation metric, we would train these models on GeForce GTX TITAN X GPUs for a fixed number of epochs, evaluating and saving our models every epoch, and would choose the best model according our to our evaluation metric. This is the approach taken in other papers faced with the same problem \cite{bahdanau_neural_2014}, \cite{barone_parallel_2017}.  If we were running a number of experiments conjointly all would run for the same number of epochs. In the longest cases (150 epochs with the longest sequences) some models ran for a day and half.

In the case of our Rote Learner, we would run each model directly on the tokenized data 50 times, to get a sense of the variance according the inherent stochasticity of the model.



% subsection evaluation_procedure (end)

% subsection tokenizing_code_features (end)

% subsubsection tokenizing_argument_descriptions (end)

% subsection tokenizing_textual_input (end)

% \subsubsection{Preparing the Data} % (fold)
% \label{ssub:Preparing the Data}

% \begin{enumerate}
%     \item All our data would come from one of the partitions of the dataset described in section \ref{sec:final_preparations}.

%     \item To tokenize variable names, function names and other arguments:
%     \begin{enumerate}
%         \item we generated a vocabulary of all possible tokens in valid python
%         \item We added separator tokens to the vocabulary, \mintinline{python}{"<SEPARATOR_1>"}
%         \item We then tokenizer as sequences of characters and adding an \mintinline{python}{"<END_OF_ARG>"} token to the end.
%     \end{enumerate}
    
%     \item To tokenize the argument descriptions:
%     \begin{enumerate}
%         \item We then generated a vocabulary for our training data by:
%         \begin{enumerate}
%             \item first generating a provisional vocabulary of tokens used more than 4 times in the training data.
%             \item if these words existed in our Glove embedding vocabulary they were added to our the final vocabulary. We used the  \mintinline{python}{glove.6B.200d.txt} file of 200d embeddings trained on 6 billion tokens for our Glove embeddings, (CITE). 
%             \item when this provisional list was exhausted we took the remaining most popular words, as defined by the Glove embedding file, to fill our final vocabulary to our designated "vocab-size"
%         \end{enumerate}
%         \item We then tokenized the descriptions by moving to lower case, removing new lines, and  tokenizing them using the nltk punkt tokenizer (\mintinline{python}{nltk.word_tokenize})
%         \item We then replaced out of vocabulary tokens with an \mintinline{python}{"<UNK>"} token, and finally bookending our descriptions with \mintinline{python}{"<START>"} and \mintinline{python}{"<END>"} tokens.
%         \item Path - seq?

%     \end{enumerate}



%     \item In cases of tokenized code: 
%      \begin{enumerate}
%          \item  In the case of tokenizing code paths, we first extracted the syntax tree from the source code, and traversed it exploring all paths between two nodes.
%          \item CONTINUE
%      \end{enumerate}

% \end{enumerate}

% Once these tokenizations were complete we were ready to train our models.

% \subsubsection{Training} % (fold)
% \label{ssub:training}


%     Our models were trained in the standard way:
%     \begin{enumerate}
%         \item The Rote Learner was trained determininstically, in this case using `ngram overlap' feature, as mentioned in\ref{sec:rote_learner_model}. It was evaluated with different random seeds 50 times on the hold out dataset.
%         \item The Seq-to-Seq model was trained using the backpropagation algorithm and adaptive momentum optimisation. The hyperparameters are presented in table X. 80 Epochs were evaluated, and the best and average scores were taken \textbf{EDIT}
%     \end{enumerate}




% \subsubsection{Evaluation} % (fold)
% \label{ssub:evaluation}

% % subsection training (end
% \begin{enumerate}



%     \item Finally when it came to evaluating our models on held-out datasets, we would ran translations on the dataset and calculated the BLEU score using the BLEU method found in \textbf{HERE}.
%     In particular BLEU scores were calculated over the whole corpus and \textbf{WHAT SMOOTHING HYPERPARAMTERS}.
%     \item It should be stressed that if a hyperparameter had to be tuned, it was always done on the validation set, with the test set untouched, until final evaluation. In the case of all neural models, the results presented are using the best hyperparameters under the validation set.

% \end{enumerate}








