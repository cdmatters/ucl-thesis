
@article{luong_effective_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.04025},
  primaryClass = {cs},
  title = {Effective {{Approaches}} to {{Attention}}-Based {{Neural Machine Translation}}},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  language = {en},
  journal = {arXiv:1508.04025 [cs]},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  month = aug,
  year = {2015},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/D9GSCUM8/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf},
  annote = {Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details}
}

@article{bahdanau_neural_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.0473},
  primaryClass = {cs, stat},
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder\textendash{}decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\textendash{}decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  language = {en},
  journal = {arXiv:1409.0473 [cs, stat]},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/erichambro/Zotero/storage/4RSKGU6J/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf},
  annote = {Comment: Accepted at ICLR 2015 as oral presentation}
}

@article{kalchbrenner_recurrent_nodate,
  title = {Recurrent {{Continuous Translation Models}}},
  abstract = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is $>$ 43\% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  language = {en},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  pages = {10},
  file = {/Users/erichambro/Zotero/storage/GX3KHIIC/Kalchbrenner and Blunsom - Recurrent Continuous Translation Models.pdf}
}

@article{ghader_what_nodate,
  title = {What Does {{Attention}} in {{Neural Machine Translation Pay Attention}} To?},
  abstract = {Attention in neural machine translation provides the possibility to encode relevant parts of the source sentence at each translation step. As a result, attention is considered to be an alignment model as well. However, there is no work that specifically studies attention and provides analysis of what is being learned by attention models. Thus, the question still remains that how attention is similar or different from the traditional alignment. In this paper, we provide detailed analysis of attention and compare it to traditional alignment. We answer the question of whether attention is only capable of modelling translational equivalent or it captures more information. We show that attention is different from alignment in some cases and is capturing useful information other than alignments.},
  language = {en},
  author = {Ghader, Hamidreza and Monz, Christof},
  pages = {10},
  file = {/Users/erichambro/Zotero/storage/KDL8ZRTH/Ghader and Monz - What does Attention in Neural Machine Translation .pdf}
}

@article{chaganty_price_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.02202},
  primaryClass = {cs},
  title = {The Price of Debiasing Automatic Metrics in Natural Language Evaluation},
  abstract = {For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7\textendash{}13\% cost reduction on evaluating summarization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks\textemdash{}the automatic metric and the prompt shown to human evaluators\textemdash{}both of which need to be improved to obtain greater cost savings.},
  language = {en},
  journal = {arXiv:1807.02202 [cs]},
  author = {Chaganty, Arun Tejasvi and Mussman, Stephen and Liang, Percy},
  month = jul,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/VAEX8FQV/Chaganty et al. - 2018 - The price of debiasing automatic metrics in natura.pdf},
  annote = {Comment: To appear ACL 2018}
}

@article{movshovitz-attias_natural_nodate,
  title = {Natural {{Language Models}} for {{Predicting Programming Comments}}},
  abstract = {Statistical language models have successfully been used to describe and analyze natural language documents. Recent work applying language models to programming languages is focused on the task of predicting code, while mainly ignoring the prediction of programmer comments. In this work, we predict comments from JAVA source files of open source projects, using topic models and n-grams, and we analyze the performance of the models given varying amounts of background data on the project being predicted. We evaluate models on their comment-completion capability in a setting similar to codecompletion tools built into standard code editors, and show that using a comment completion tool can save up to 47\% of the comment typing.},
  language = {en},
  author = {{Movshovitz-Attias}, Dana and Cohen, William W},
  pages = {6},
  file = {/Users/erichambro/Zotero/storage/NMS92QHI/Movshovitz-Attias and Cohen - Natural Language Models for Predicting Programming.pdf}
}

@inproceedings{nguyen_statistical_2013,
  address = {Saint Petersburg, Russia},
  title = {A Statistical Semantic Language Model for Source Code},
  isbn = {978-1-4503-2237-9},
  doi = {10.1145/2491411.2491458},
  abstract = {Recent research has successfully applied the statistical ngram language model to show that source code exhibits a good level of repetition. The n-gram model is shown to have good predictability in supporting code suggestion and completion. However, the state-of-the-art n-gram approach to capture source code regularities/patterns is based only on the lexical information in a local context of the code units. To improve predictability, we introduce SLAMC, a novel statistical semantic language model for source code. It incorporates semantic information into code tokens and models the regularities/patterns of such semantic annotations, called sememes, rather than their lexemes. It combines the local context in semantic n-grams with the global technical concerns/ functionality into an n-gram topic model, together with pairwise associations of program elements. Based on SLAMC, we developed a new code suggestion method, which is empirically evaluated on several projects to have relatively 18\textendash{}68\% higher accuracy than the state-of-the-art approach.},
  language = {en},
  booktitle = {Proceedings of the 2013 9th {{Joint Meeting}} on {{Foundations}} of {{Software Engineering}} - {{ESEC}}/{{FSE}} 2013},
  publisher = {{ACM Press}},
  author = {Nguyen, Tung Thanh and Nguyen, Anh Tuan and Nguyen, Hoan Anh and Nguyen, Tien N.},
  year = {2013},
  pages = {532},
  file = {/Users/erichambro/Zotero/storage/4SDSUGCE/Nguyen et al. - 2013 - A statistical semantic language model for source c.pdf}
}

@inproceedings{karaivanov_phrase-based_2014,
  address = {Portland, Oregon, USA},
  title = {Phrase-{{Based Statistical Translation}} of {{Programming Languages}}},
  isbn = {978-1-4503-3210-1},
  doi = {10.1145/2661136.2661148},
  abstract = {Phrase-based statistical machine translation approaches have been highly successful in translating between natural languages and are heavily used by commercial systems (e.g. Google Translate).},
  language = {en},
  booktitle = {Proceedings of the 2014 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} \& {{Software}} - {{Onward}}! '14},
  publisher = {{ACM Press}},
  author = {Karaivanov, Svetoslav and Raychev, Veselin and Vechev, Martin},
  year = {2014},
  pages = {173-184},
  file = {/Users/erichambro/Zotero/storage/479BZVYI/Karaivanov et al. - 2014 - Phrase-Based Statistical Translation of Programmin.pdf}
}

@inproceedings{iyer_summarizing_2016,
  address = {Berlin, Germany},
  title = {Summarizing {{Source Code}} Using a {{Neural Attention Model}}},
  doi = {10.18653/v1/P16-1195},
  abstract = {High quality source code is often paired with high level summaries of the computation it performs, for example in code documentation or in descriptions posted in online forums. Such summaries are extremely useful for applications such as code search but are expensive to manually author, hence only done for a small fraction of all code that is produced. In this paper, we present the first completely datadriven approach for generating high level summaries of source code. Our model, CODE-NN , uses Long Short Term Memory (LSTM) networks with attention to produce sentences that describe C\# code snippets and SQL queries. CODE-NN is trained on a new corpus that is automatically collected from StackOverflow, which we release. Experiments demonstrate strong performance on two tasks: (1) code summarization, where we establish the first end-to-end learning results and outperform strong baselines, and (2) code retrieval, where our learned model improves the state of the art on a recently introduced C\# benchmark by a large margin.},
  language = {en},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  publisher = {{Association for Computational Linguistics}},
  author = {Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
  year = {2016},
  pages = {2073-2083},
  file = {/Users/erichambro/Zotero/storage/C49SKRPS/Iyer et al. - 2016 - Summarizing Source Code using a Neural Attention M.pdf}
}

@article{allamanis_convolutional_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.03001},
  primaryClass = {cs},
  title = {A {{Convolutional Attention Network}} for {{Extreme Summarization}} of {{Source Code}}},
  abstract = {Attention mechanisms in neural networks have proved useful for problems in which the input and output do not have fixed dimension. Often there exist features that are locally translation invariant and would be valuable for directing the model's attention, but previous attentional architectures are not constructed to learn such features specifically. We introduce an attentional neural network that employs convolution on the input tokens to detect local time-invariant and long-range topical attention features in a context-dependent way. We apply this architecture to the problem of extreme summarization of source code snippets into short, descriptive function name-like summaries. Using those features, the model sequentially generates a summary by marginalizing over two attention mechanisms: one that predicts the next summary token based on the attention weights of the input tokens and another that is able to copy a code token as-is directly into the summary. We demonstrate our convolutional attention neural network's performance on 10 popular Java projects showing that it achieves better performance compared to previous attentional mechanisms.},
  language = {en},
  journal = {arXiv:1602.03001 [cs]},
  author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
  month = feb,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/erichambro/Zotero/storage/7GIUG67L/Allamanis et al. - 2016 - A Convolutional Attention Network for Extreme Summ.pdf},
  annote = {Comment: Code, data and visualization at http://groups.inf.ed.ac.uk/cup/codeattention/}
}

@inproceedings{raychev_predicting_2015,
  address = {Mumbai, India},
  title = {Predicting {{Program Properties}} from "{{Big Code}}"},
  isbn = {978-1-4503-3300-9},
  doi = {10.1145/2676726.2677009},
  abstract = {We present a new approach for predicting program properties from massive codebases (aka ``Big Code''). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs.},
  language = {en},
  booktitle = {Proceedings of the 42nd {{Annual ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}} - {{POPL}} '15},
  publisher = {{ACM Press}},
  author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
  year = {2015},
  pages = {111-124},
  file = {/Users/erichambro/Zotero/storage/Q3NAJQRM/Raychev et al. - 2015 - Predicting Program Properties from Big Code.pdf}
}

@article{allamanis_learning_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00740},
  primaryClass = {cs},
  title = {Learning to {{Represent Programs}} with {{Graphs}}},
  abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known sematics. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures.},
  language = {en},
  journal = {arXiv:1711.00740 [cs]},
  author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  month = nov,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/SDJUV5PZ/Allamanis et al. - 2017 - Learning to Represent Programs with Graphs.pdf},
  annote = {Comment: Published in ICLR 2018. arXiv admin note: text overlap with arXiv:1705.07867}
}

@article{bhoopchand_learning_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.08307},
  primaryClass = {cs},
  title = {Learning {{Python Code Suggestion}} with a {{Sparse Pointer Network}}},
  abstract = {To enhance developer productivity, all modern integrated development environments (IDEs) include code suggestion functionality that proposes likely next tokens at the cursor. While current IDEs work well for statically-typed languages, their reliance on type annotations means that they do not provide the same level of support for dynamic programming languages as for statically-typed languages. Moreover, suggestion engines in modern IDEs do not propose expressions or multi-statement idiomatic code. Recent work has shown that language models can improve code suggestion systems by learning from software repositories. This paper introduces a neural language model with a sparse pointer network aimed at capturing very longrange dependencies. We release a large-scale code suggestion corpus of 41M lines of Python code crawled from GitHub. On this corpus, we found standard neural language models to perform well at suggesting local phenomena, but struggle to refer to identifiers that are introduced many tokens in the past. By augmenting a neural language model with a pointer network specialized in referring to predefined classes of identifiers, we obtain a much lower perplexity and a 5 percentage points increase in accuracy for code suggestion compared to an LSTM baseline. In fact, this increase in code suggestion accuracy is due to a 13 times more accurate prediction of identifiers. Furthermore, a qualitative analysis shows this model indeed captures interesting long-range dependencies, like referring to a class member defined over 60 tokens in the past.},
  language = {en},
  journal = {arXiv:1611.08307 [cs]},
  author = {Bhoopchand, Avishkar and Rockt\"aschel, Tim and Barr, Earl and Riedel, Sebastian},
  month = nov,
  year = {2016},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Computer Science - Software Engineering,Computer Science - Artificial Intelligence},
  file = {/Users/erichambro/Zotero/storage/BS9NQECI/Bhoopchand et al. - 2016 - Learning Python Code Suggestion with a Sparse Poin.pdf},
  annote = {Comment: Under review as a conference paper at ICLR 2017}
}

@article{allamanis_mining_nodate,
  title = {Mining {{Semantic Loop Idioms}} from {{Big Code}}},
  abstract = {During maintenance, developers spend a lot of time transforming existing code: refactoring, optimizing, and adding checks to make it more robust. Much of this work is the drudgery of identifying and replacing specific patterns, yet it resists automation, because of meaningful patterns are hard to automatically find. We present a technique for mining loop idioms, surprisingly probable semantic patterns that occur in loops, from big code to find meaningful patterns. First, we show that automatically identifiable patterns exist, in great numbers, with a large scale empirical study of loop over 25 MLOC. We find that loops in this corpus are simple and predictable: 90\% of them have fewer than 15LOC and 90\% have no nesting and very simple control structure. Encouraged by this result, we coil loops to abstract away syntactic diversity to define information rich loop idioms. We show that only 50 loop idioms cover 50\% of the concrete loops. We show how loop idioms can help a tool developers identify and prioritize refactorings. We also show how our framework opens the door to data-driven tool and language design discovering opportunities to introduce new API calls and language constructs: loop idioms show that LINQ would benefit from an Enumerate operator, a result confirmed by the fact that precisely this feature is one of the most requested features on StackOverflow with 197 votes and 95k views.},
  language = {en},
  author = {Allamanis, Miltiadis and Devanbu, Premkumar and Barr, Earl T and Marron, Mark},
  pages = {20},
  file = {/Users/erichambro/Zotero/storage/9DT5T9XR/Allamanis et al. - Mining Semantic Loop Idioms from Big Code.pdf}
}

@article{alon_code2vec_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09473},
  primaryClass = {cs, stat},
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  shorttitle = {Code2vec},
  abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent code as a collection of paths in its abstract syntax tree, and aggregate these paths, in a smart and scalable way, into a single fixed-length code vector, which can be used to predict semantic properties of the snippet. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75\%, being the first to successfully predict method names based on a large, cross-project, corpus.},
  language = {en},
  journal = {arXiv:1803.09473 [cs, stat]},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/TZWRQSP6/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf}
}

@article{alon_code2seq_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.01400},
  primaryClass = {cs, stat},
  title = {Code2seq: {{Generating Sequences}} from {{Structured Representations}} of {{Code}}},
  shorttitle = {Code2seq},
  abstract = {The ability to generate natural language sequences from source code snippets can be used for code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of paths in its abstract syntax tree (AST) and uses attention to select the relevant paths during decoding, much like contemporary NMT models. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.},
  language = {en},
  journal = {arXiv:1808.01400 [cs, stat]},
  author = {Alon, Uri and Levy, Omer and Yahav, Eran},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/UMSJDARJ/Alon et al. - 2018 - code2seq Generating Sequences from Structured Rep.pdf}
}

@inproceedings{shi_why_2016,
  address = {Austin, Texas},
  title = {Why {{Neural Translations}} Are the {{Right Length}}},
  doi = {10.18653/v1/D16-1248},
  abstract = {We investigate how neural, encoder-decoder translation systems output target strings of appropriate lengths, finding that a collection of hidden units learns to explicitly implement this functionality.},
  language = {en},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  author = {Shi, Xing and Knight, Kevin and Yuret, Deniz},
  year = {2016},
  pages = {2278-2282},
  file = {/Users/erichambro/Zotero/storage/FM7HE4DZ/Shi et al. - 2016 - Why Neural Translations are the Right Length.pdf}
}

@article{noraset_definition_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.00394},
  primaryClass = {cs},
  title = {Definition {{Modeling}}: {{Learning}} to Define Word Embeddings in Natural Language},
  shorttitle = {Definition {{Modeling}}},
  abstract = {Distributed representations of words have been shown to capture lexical semantics, as demonstrated by their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present several definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a characterlevel convolution layer designed to leverage morphology can complement word-level embeddings. Finally, an error analysis suggests that the errors made by a definition model may provide insight into the shortcomings of word embeddings.},
  language = {en},
  journal = {arXiv:1612.00394 [cs]},
  author = {Noraset, Thanapon and Liang, Chen and Birnbaum, Larry and Downey, Doug},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/PUYZ3SVR/Noraset et al. - 2016 - Definition Modeling Learning to define word embed.pdf},
  annote = {Comment: To appear in AAAI Conference 2017}
}

@article{alon_code2vec_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09473},
  primaryClass = {cs, stat},
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  shorttitle = {Code2vec},
  abstract = {We present a neural model for representing snippets of code as continuous distributed vectors. The main idea is to represent code as a collection of paths in its abstract syntax tree, and aggregate these paths, in a smart and scalable way, into a single fixed-length \$$\backslash$textit\{code vector\}\$, which can be used to predict semantic properties of the snippet. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of \$14\$M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over \$75$\backslash$\%\$, being the first to successfully predict method names based on a large, cross-project, corpus.},
  journal = {arXiv:1803.09473 [cs, stat]},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/2I3B96SA/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf;/Users/erichambro/Zotero/storage/V3FAY7TY/1803.html}
}

@article{alon_code2vec_2018-2,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09473},
  primaryClass = {cs, stat},
  title = {Code2vec: {{Learning Distributed Representations}} of {{Code}}},
  shorttitle = {Code2vec},
  abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent code as a collection of paths in its abstract syntax tree, and aggregate these paths, in a smart and scalable way, into a single fixed-length code vector, which can be used to predict semantic properties of the snippet. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 14M methods. We show that code vectors trained on this dataset can predict method names from files that were completely unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. Comparing previous techniques over the same data set, our approach obtains a relative improvement of over 75\%, being the first to successfully predict method names based on a large, cross-project, corpus.},
  language = {en},
  journal = {arXiv:1803.09473 [cs, stat]},
  author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/5QZMXH8H/Alon et al. - 2018 - code2vec Learning Distributed Representations of .pdf}
}

@article{alon_code2seq_2018-1,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.01400},
  primaryClass = {cs, stat},
  title = {Code2seq: {{Generating Sequences}} from {{Structured Representations}} of {{Code}}},
  shorttitle = {Code2seq},
  abstract = {The ability to generate natural language sequences from source code snippets can be used for code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation (NMT), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present CODE2SEQ: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of paths in its abstract syntax tree (AST) and uses attention to select the relevant paths during decoding, much like contemporary NMT models. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to 16M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as general state-of-the-art NMT models.},
  language = {en},
  journal = {arXiv:1808.01400 [cs, stat]},
  author = {Alon, Uri and Levy, Omer and Yahav, Eran},
  month = aug,
  year = {2018},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/SQTHPGG7/Alon et al. - 2018 - code2seq Generating Sequences from Structured Rep.pdf}
}

@article{oda_learning_nodate,
  title = {Learning to {{Generate Pseudo}}-Code from {{Source Code}} Using {{Statistical Machine Translation}}},
  abstract = {Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.},
  language = {en},
  author = {Oda, Yusuke and Fudaba, Hiroyuki and Neubig, Graham and Hata, Hideaki and Sakti, Sakriani and Toda, Tomoki and Nakamura, Satoshi},
  pages = {11},
  file = {/Users/erichambro/Zotero/storage/BVJHLFET/Oda et al. - Learning to Generate Pseudo-code from Source Code .pdf}
}

@article{barzilay_using_nodate,
  title = {Using {{Semantic Unification}} to {{Generate Regular Expressions}} from {{Natural Language}}},
  language = {en},
  author = {Barzilay, Nate Kushman Regina},
  pages = {11},
  file = {/Users/erichambro/Zotero/storage/LFXAUCGB/Barzilay - Using Semantic Uniﬁcation to Generate Regular Expr.pdf}
}

@article{bessey_few_2010,
  title = {A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs in the Real World},
  volume = {53},
  issn = {00010782},
  shorttitle = {A Few Billion Lines of Code Later},
  doi = {10.1145/1646353.1646374},
  language = {en},
  number = {2},
  journal = {Communications of the ACM},
  author = {Bessey, Al and Engler, Dawson and Block, Ken and Chelf, Ben and Chou, Andy and Fulton, Bryan and Hallem, Seth and {Henri-Gros}, Charles and Kamsky, Asya and McPeak, Scott},
  month = feb,
  year = {2010},
  pages = {66-75},
  file = {/Users/erichambro/Zotero/storage/9YK4X6SC/Bessey et al. - 2010 - A few billion lines of code later using static an.pdf}
}

@incollection{okada_combination_2007,
  address = {Berlin, Heidelberg},
  title = {Combination of {{Abstractions}} in the {{ASTR\'EE Static Analyzer}}},
  volume = {4435},
  isbn = {978-3-540-77504-1 978-3-540-77505-8},
  abstract = {We describe the structure of the abstract domains in the Astre\textasciiacute{}e static analyzer, their modular organization into a hierarchical network, their cooperation to over-approximate the conjunction/reduced product of different abstractions and to ensure termination using collaborative widenings and narrowings. This separation of the abstraction into a combination of cooperative abstract domains makes Astre\textasciiacute{}e extensible, an essential feature to cope with false alarms and ultimately provide sound formal verification of the absence of runtime errors in very large software.},
  language = {en},
  booktitle = {Advances in {{Computer Science}} - {{ASIAN}} 2006. {{Secure Software}} and {{Related Issues}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Cousot, Patrick and Cousot, Radhia and Feret, J\'er\^ome and Mauborgne, Laurent and Min\'e, Antoine and Monniaux, David and Rival, Xavier},
  editor = {Okada, Mitsu and Satoh, Ichiro},
  year = {2007},
  pages = {272-300},
  file = {/Users/erichambro/Zotero/storage/N629XJM4/Cousot et al. - 2007 - Combination of Abstractions in the ASTRÉE Static A.pdf},
  doi = {10.1007/978-3-540-77505-8_23}
}

@misc{knuth_donald_e._knuthweb.pdf_nodate,
  title = {Knuthweb.Pdf},
  howpublished = {http://dx.doi.org/10.1093/comjnl/27.2.97},
  author = {Knuth, Donald E.},
  file = {/Users/erichambro/Zotero/storage/GU3M6LBI/knuthweb.pdf}
}

@article{knuth_literate_1984,
  title = {Literate {{Programming}}},
  volume = {27},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. This paper presents WEB by example, and discusses why the new system appears to be an improvement over previous ones.},
  number = {2},
  journal = {The Computer Journal},
  author = {Knuth, D. E.},
  month = jan,
  year = {1984},
  pages = {97-111},
  annote = {10.1093/comjnl/27.2.97}
}

@inproceedings{floyd_decoding_2017,
  title = {Decoding the {{Representation}} of {{Code}} in the {{Brain}}: {{An fMRI Study}} of {{Code Review}} and {{Expertise}}},
  shorttitle = {Decoding the {{Representation}} of {{Code}} in the {{Brain}}},
  doi = {10.1109/ICSE.2017.24},
  abstract = {Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79\%, p $<$; 0.001). Further, we find that the same set of brain regions distinguish between code and prose (near-perfect correlation, r = 0.99, p $<$; 0.001). Finally, we find that task distinctions are modulated by expertise, such that greater skill predicts a less differentiated neural representation (r = -0.44, p = 0.016) indicating that more skilled participants treat code and prose more similarly at a neural activation level.},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Floyd, B. and Santander, T. and Weimer, W.},
  month = may,
  year = {2017},
  keywords = {Biomedical imaging,biomedical MRI,Brain,code comprehension,code review,Computer science,fMRI study,functional magnetic resonance imaging,medical image processing,medical imaging,medical imaging techniques,natural languages,Natural languages,neural representations,programming languages,prose review,Software,software engineering,Software engineering,software engineering tasks,Tools},
  pages = {175-186},
  file = {/Users/erichambro/Zotero/storage/RGY4LN3T/Floyd et al. - 2017 - Decoding the Representation of Code in the Brain .pdf;/Users/erichambro/Zotero/storage/3ZUDX2UK/7985660.html}
}

@article{hindle_naturalness_nodate,
  title = {On the Naturalness of Software},
  abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations\textemdash{}and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.},
  language = {en},
  author = {Hindle, Abram and Barr, Earl T and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
  pages = {11},
  file = {/Users/erichambro/Zotero/storage/E85YXJAP/Hindle et al. - On the naturalness of software.pdf}
}

@inproceedings{sridhara_[not_2010,
  address = {Antwerp, Belgium},
  title = {[{{NOT NATURALNESS}}] {{Towards}} Automatically Generating Summary Comments for {{Java}} Methods},
  isbn = {978-1-4503-0116-9},
  doi = {10.1145/1858996.1859006},
  language = {en},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} International Conference on {{Automated}} Software Engineering - {{ASE}} '10},
  publisher = {{ACM Press}},
  author = {Sridhara, Giriprasad and Hill, Emily and Muppaneni, Divya and Pollock, Lori and {Vijay-Shanker}, K.},
  year = {2010},
  pages = {43},
  file = {/Users/erichambro/Zotero/storage/5LNWWVKR/Sridhara et al. - 2010 - Towards automatically generating summary comments .pdf}
}

@inproceedings{sridhara_[not_2011,
  address = {Waikiki, Honolulu, HI, USA},
  title = {[{{NOT NATURALNESS}}] {{Automatically}} Detecting and Describing High Level Actions within Methods},
  isbn = {978-1-4503-0445-0},
  doi = {10.1145/1985793.1985808},
  abstract = {One approach to easing program comprehension is to reduce the amount of code that a developer has to read. Describing the high level abstract algorithmic actions associated with code fragments using succinct natural language phrases potentially enables a newcomer to focus on fewer and more abstract concepts when trying to understand a given method. Unfortunately, such descriptions are typically missing because it is tedious to create them manually.},
  language = {en},
  booktitle = {Proceeding of the 33rd International Conference on {{Software}} Engineering - {{ICSE}} '11},
  publisher = {{ACM Press}},
  author = {Sridhara, Giriprasad and Pollock, Lori and {Vijay-Shanker}, K.},
  year = {2011},
  pages = {101},
  file = {/Users/erichambro/Zotero/storage/MZ5HUG46/Sridhara et al. - 2011 - Automatically detecting and describing high level .pdf}
}

@inproceedings{buse_[not_2010,
  address = {Antwerp, Belgium},
  title = {[{{NOT NATURALNESS}}] {{Automatically}} Documenting Program Changes},
  isbn = {978-1-4503-0116-9},
  doi = {10.1145/1858996.1859005},
  abstract = {Source code modifications are often documented with log messages. Such messages are a key component of software maintenance: they can help developers validate changes, locate and triage defects, and understand modifications. However, this documentation can be burdensome to create and can be incomplete or inaccurate.},
  language = {en},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} International Conference on {{Automated}} Software Engineering - {{ASE}} '10},
  publisher = {{ACM Press}},
  author = {Buse, Raymond P.L. and Weimer, Westley R.},
  year = {2010},
  pages = {33},
  file = {/Users/erichambro/Zotero/storage/V5UMSCFF/Buse and Weimer - 2010 - Automatically documenting program changes.pdf},
  annote = {AUTOMATIC PROGRAM ANALYSIS not BIG CODE}
}

@inproceedings{gabel_study_2010,
  address = {Santa Fe, New Mexico, USA},
  title = {A Study of the Uniqueness of Source Code},
  isbn = {978-1-60558-791-2},
  doi = {10.1145/1882291.1882315},
  abstract = {This paper presents the results of the first study of the uniqueness of source code. We define the uniqueness of a unit of source code with respect to the entire body of written software, which we approximate with a corpus of 420 million lines of source code. Our high-level methodology consists of examining a collection of 6,000 software projects and measuring the degree to which each project can be `assembled' solely from portions of this corpus, thus providing a precise measure of `uniqueness' that we call syntactic redundancy. We parameterized our study over a variety of variables, the most important of which being the level of granularity at which we view source code. Our suite of experiments together consumed approximately four months of CPU time, providing quantitative answers to the following questions: at what levels of granularity is software unique, and at a given level of granularity, how unique is software? While we believe these questions to be of intrinsic interest, we discuss possible applications to genetic programming and developer productivity tools.},
  language = {en},
  booktitle = {Proceedings of the Eighteenth {{ACM SIGSOFT}} International Symposium on {{Foundations}} of Software Engineering - {{FSE}} '10},
  publisher = {{ACM Press}},
  author = {Gabel, Mark and Su, Zhendong},
  year = {2010},
  pages = {147},
  file = {/Users/erichambro/Zotero/storage/JUMBYXZP/Gabel and Su - 2010 - A study of the uniqueness of source code.pdf}
}

@article{tu_localness_nodate,
  title = {On the {{Localness}} of {{Software}}},
  abstract = {The n-gram language model, which has its roots in statistical natural language processing, has been shown to successfully capture the repetitive and predictable regularities (``naturalness") of source code, and help with tasks such as code suggestion, porting, and designing assistive coding devices. However, we show in this paper that this natural-language-based model fails to exploit a special property of source code: localness. We find that human-written programs are localized: they have useful local regularities that can be captured and exploited. We introduce a novel cache language model that consists of both an n-gram and an added ``cache" component to exploit localness. We show empirically that the additional cache component greatly improves the n-gram approach by capturing the localness of software, as measured by both cross-entropy and suggestion accuracy. Our model's suggestion accuracy is actually comparable to a state-of-the-art, semantically augmented language model; but it is simpler and easier to implement. Our cache language model requires nothing beyond lexicalization, and thus is applicable to all programming languages.},
  language = {en},
  author = {Tu, Zhaopeng and Su, Zhendong and Devanbu, Premkumar},
  pages = {12},
  file = {/Users/erichambro/Zotero/storage/VREW3WCB/Tu et al. - On the Localness of Software.pdf}
}

@article{loyola_neural_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04856},
  primaryClass = {cs},
  title = {A {{Neural Architecture}} for {{Generating Natural Language Descriptions}} from {{Source Code Changes}}},
  abstract = {We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.},
  language = {en},
  journal = {arXiv:1704.04856 [cs]},
  author = {Loyola, Pablo and {Marrese-Taylor}, Edison and Matsuo, Yutaka},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/erichambro/Zotero/storage/GIPBWBUC/Loyola et al. - 2017 - A Neural Architecture for Generating Natural Langu.pdf},
  annote = {Comment: Accepted at ACL 2017}
}

@inproceedings{white_toward_2015,
  address = {Florence, Italy},
  title = {Toward {{Deep Learning Software Repositories}}},
  isbn = {978-0-7695-5594-2},
  doi = {10.1109/MSR.2015.38},
  abstract = {Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields such as natural language processing (NLP). Recent research in the software engineering (SE) community has demonstrated the usefulness of applying NLP techniques to software corpora. Hence, we motivate deep learning for software language modeling, highlighting fundamental differences between state-of-the-practice software language models and connectionist models. Our deep learning models are applicable to source code files (since they only require lexically analyzed source code written in any programming language) and other types of artifacts. We show how a particular deep learning model can remember its state to effectively model sequential data, e.g., streaming software tokens, and the state is shown to be much more expressive than discrete tokens in a prefix. Then we instantiate deep learning models and show that deep learning induces high-quality models compared to n-grams and cachebased n-grams on a corpus of Java projects. We experiment with two of the models' hyperparameters, which govern their capacity and the amount of context they use to inform predictions, before building several committees of software language models to aid generalization. Then we apply the deep learning models to code suggestion and demonstrate their effectiveness at a real SE task compared to state-of-the-practice models. Finally, we propose avenues for future work, where deep learning can be brought to bear to support model-based testing, improve software lexicons, and conceptualize software artifacts. Thus, our work serves as the first step toward deep learning software repositories.},
  language = {en},
  booktitle = {2015 {{IEEE}}/{{ACM}} 12th {{Working Conference}} on {{Mining Software Repositories}}},
  publisher = {{IEEE}},
  author = {White, Martin and Vendome, Christopher and {Linares-Vasquez}, Mario and Poshyvanyk, Denys},
  month = may,
  year = {2015},
  pages = {334-345},
  file = {/Users/erichambro/Zotero/storage/C63RPBHH/White et al. - 2015 - Toward Deep Learning Software Repositories.pdf}
}

@article{allamanis_survey_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.06182},
  primaryClass = {cs},
  title = {A {{Survey}} of {{Machine Learning}} for {{Big Code}} and {{Naturalness}}},
  abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit code's abundance of patterns. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
  language = {en},
  journal = {arXiv:1709.06182 [cs]},
  author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
  month = sep,
  year = {2017},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering,Computer Science - Programming Languages},
  file = {/Users/erichambro/Zotero/storage/XBID69AZ/Allamanis et al. - 2017 - A Survey of Machine Learning for Big Code and Natu.pdf},
  annote = {Comment: Website accompanying this survey paper can be found at https://ml4code.github.io}
}

@inproceedings{allamanis_mining_2013,
  address = {San Francisco, CA, USA},
  title = {Mining Source Code Repositories at Massive Scale Using Language Modeling},
  isbn = {978-1-4673-2936-1 978-1-4799-0345-0},
  doi = {10.1109/MSR.2013.6624029},
  abstract = {The tens of thousands of high-quality open source software projects on the Internet raise the exciting possibility of studying software development by finding patterns across truly large source code repositories. This could enable new tools for developing code, encouraging reuse, and navigating large projects. In this paper, we build the first giga-token probabilistic language model of source code, based on 352 million lines of Java. This is 100 times the scale of the pioneering work by Hindle et al. The giga-token model is significantly better at the code suggestion task than previous models. More broadly, our approach provides a new ``lens'' for analyzing software projects, enabling new complexity metrics based on statistical analysis of large corpora. We call these metrics data-driven complexity metrics. We propose new metrics that measure the complexity of a code module and the topical centrality of a module to a software project. In particular, it is possible to distinguish reusable utility classes from classes that are part of a program's core logic based solely on general information theoretic criteria.},
  language = {en},
  booktitle = {2013 10th {{Working Conference}} on {{Mining Software Repositories}} ({{MSR}})},
  publisher = {{IEEE}},
  author = {Allamanis, Miltiadis and Sutton, Charles},
  month = may,
  year = {2013},
  pages = {207-216},
  file = {/Users/erichambro/Zotero/storage/5D32EAMW/Allamanis and Sutton - 2013 - Mining source code repositories at massive scale u.pdf}
}

@article{barone_parallel_2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.02275},
  primaryClass = {cs},
  title = {A Parallel Corpus of {{Python}} Functions and Documentation Strings for Automated Code Documentation and Code Generation},
  abstract = {Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains.},
  language = {en},
  journal = {arXiv:1707.02275 [cs]},
  author = {Barone, Antonio Valerio Miceli and Sennrich, Rico},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Artificial Intelligence},
  file = {/Users/erichambro/Zotero/storage/F6XMRWZG/Barone and Sennrich - 2017 - A parallel corpus of Python functions and document.pdf},
  annote = {Comment: 5 pages, 1 figure, 3 tables}
}

@article{hochreiter_long_1997,
  title = {Long {{Short}}-{{Term Memory}}},
  volume = {9},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  number = {8},
  journal = {Neural Computation},
  author = {Hochreiter, Sepp and Schmidhuber, J\"urgen},
  month = nov,
  year = {1997},
  pages = {1735-1780},
  annote = {doi: 10.1162/neco.1997.9.8.1735}
}

@misc{noauthor_numpydoc_nodate,
  title = {Numpydoc Docstring Guide \textemdash{} Numpydoc v0.9.Dev0 {{Manual}}},
  howpublished = {https://numpydoc.readthedocs.io/en/latest/format.html\#import-conventions},
  file = {/Users/erichambro/Zotero/storage/R55NST9C/format.html}
}

@misc{noauthor_style_2018,
  title = {Style Guides for {{Google}}-Originated Open-Source Projects: Google/Styleguide},
  shorttitle = {Style Guides for {{Google}}-Originated Open-Source Projects},
  organization = {{Google}},
  month = aug,
  year = {2018}
}

@inproceedings{dyer_boa_2013,
  address = {San Francisco, CA, USA},
  title = {Boa: {{A}} Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories},
  isbn = {978-1-4673-3076-3 978-1-4673-3073-2},
  shorttitle = {Boa},
  doi = {10.1109/ICSE.2013.6606588},
  abstract = {In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers.},
  language = {en},
  booktitle = {2013 35th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  publisher = {{IEEE}},
  author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
  month = may,
  year = {2013},
  pages = {422-431},
  file = {/Users/erichambro/Zotero/storage/JVJ4LBPM/Dyer et al. - 2013 - Boa A language and infrastructure for analyzing u.pdf}
}


